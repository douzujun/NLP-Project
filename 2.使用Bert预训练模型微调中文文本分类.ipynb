{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch-使用Bert预训练模型微调中文文本分类.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP5eyOdl/S2VQKOeAW1zZ87",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/douzujun/NLP-Project/blob/master/%E4%BD%BF%E7%94%A8Bert%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9q5Kcu1yQPY",
        "colab_type": "text"
      },
      "source": [
        "语料链接：https://pan.baidu.com/s/1YxGGYmeByuAlRdAVov_ZLg\n",
        "提取码：tzao\n",
        "\n",
        "neg.txt和pos.txt各5000条酒店评论，每条评论一行。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoH9VrHLyFiY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "b10075de-6ae3-4620-ab37-19de8bff3254"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry0PkCK2yerw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c424b73f-0b46-4cf1-ad81-60ff1732111a"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import matplotlib.pylab as plt \n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "SEED = 123\n",
        "BATCH_SIZE = 16\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 1e-2  # 0.01\n",
        "epsilon = 1e-8\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f7265d12060>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gp4_kgv_MEb",
        "colab_type": "text"
      },
      "source": [
        "# 1. 数据预处理\n",
        "\n",
        "## 1.1 读取文件\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6-4feGA2OtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readFile(filename):\n",
        "    with open(filename, encoding='utf-8') as f:\n",
        "        content = f.readlines()\n",
        "        return content\n",
        "\n",
        "pos_text, neg_text = readFile('./sample_data/pos.txt'), readFile('./sample_data/neg.txt')\n",
        "sentences = pos_text + neg_text\n",
        "\n",
        "# 设定标签\n",
        "pos_targets = np.ones([len(pos_text)])  # (5000, )\n",
        "neg_targets = np.zeros([len(neg_text)]) # (5000, )\n",
        "targets = np.concatenate((pos_targets, neg_targets), axis=0).reshape(-1, 1) # (10000, 1)\n",
        "total_targets = torch.tensor(targets)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqtLQtD5_pXN",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 BertTokenizer进行编码，将每一句转成数字"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OViCya5Q2Q2j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "40548311-d5fa-4824-c1fd-d6fe0350fefa"
      },
      "source": [
        "model_name = 'bert-base-chinese'\n",
        "cache_dir = './sample_data/'\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
        "print(pos_text[2])\n",
        "print(tokenizer.tokenize(pos_text[2]))\n",
        "print(tokenizer.encode(pos_text[2]))\n",
        "print(tokenizer.convert_ids_to_tokens(tokenizer.encode(pos_text[2])))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "不错，下次还考虑入住。交通也方便，在餐厅吃的也不错。\n",
            "\n",
            "['不', '错', '，', '下', '次', '还', '考', '虑', '入', '住', '。', '交', '通', '也', '方', '便', '，', '在', '餐', '厅', '吃', '的', '也', '不', '错', '。']\n",
            "[101, 679, 7231, 8024, 678, 3613, 6820, 5440, 5991, 1057, 857, 511, 769, 6858, 738, 3175, 912, 8024, 1762, 7623, 1324, 1391, 4638, 738, 679, 7231, 511, 102]\n",
            "['[CLS]', '不', '错', '，', '下', '次', '还', '考', '虑', '入', '住', '。', '交', '通', '也', '方', '便', '，', '在', '餐', '厅', '吃', '的', '也', '不', '错', '。', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5ExQ8ji2Q8f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "20255e57-078d-4d58-a707-f79300d8a0ac"
      },
      "source": [
        "# 将每一句转成数字 （大于126做截断，小于126做 Padding，加上首位两个标识，长度总共等于128）\n",
        "def convert_text_to_token(tokenizer, sentence, limit_size = 126):\n",
        "    tokens = tokenizer.encode(sentence[:limit_size])       # 直接截断\n",
        "    if len(tokens) < limit_size + 2:                       # 补齐（pad的索引号就是0）\n",
        "        tokens.extend([0] * (limit_size + 2 - len(tokens)))\n",
        "    return tokens\n",
        "\n",
        "input_ids = [convert_text_to_token(tokenizer, sen) for sen in sentences]\n",
        "\n",
        "input_tokens = torch.tensor(input_ids)\n",
        "print(input_tokens.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10000, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeYRZQqRTVLS",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 attention_masks, 在一个文本中，如果是PAD符号则是0，否则就是1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX2hM7hW2RFp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8d6b2a04-bf63-4c11-fa7a-ea14f842dc11"
      },
      "source": [
        "# 建立mask\n",
        "def attention_masks(input_ids):\n",
        "    atten_masks = []\n",
        "    for seq in input_ids:                       # [10000, 128]\n",
        "        seq_mask = [float(i > 0) for i in seq]  # PAD: 0; 否则: 1\n",
        "        atten_masks.append(seq_mask)\n",
        "    return atten_masks\n",
        "\n",
        "atten_masks = attention_masks(input_ids)\n",
        "attention_tokens = torch.tensor(atten_masks)\n",
        "print(attention_tokens.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10000, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yc1nwTdXC50",
        "colab_type": "text"
      },
      "source": [
        "- 构造input_ids和atten_masks的目的和前面一节中提到的.encode_plus函数返回的input_ids和attention_mask一样\n",
        "\n",
        "- input_type_ids和本次任务无关，它是针对每个训练集有两个句子的任务（如问答任务）。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ottvMKol2RLE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "outputId": "fcec2cae-960b-4b63-d8fa-3720bf5ac75d"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_inputs, test_inputs, train_labels, test_labels = train_test_split(input_tokens, total_targets, \n",
        "                                                    random_state=666, test_size=0.2)\n",
        "train_masks, test_masks, _, _ = train_test_split(attention_tokens, input_tokens, \n",
        "                           random_state=666, test_size=0.2)\n",
        "print(train_inputs.shape, test_inputs.shape)     # torch.Size([8000, 128]) torch.Size([2000, 128])\n",
        "print(train_masks.shape)                # torch.Size([8000, 128]) 和 train_inputs形状一样\n",
        "\n",
        "print(train_inputs[0])\n",
        "print(train_masks[0])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8000, 128]) torch.Size([2000, 128])\n",
            "torch.Size([8000, 128])\n",
            "tensor([  101,  2769,  6370,  4638,  3221, 10189,  1039,  4638,   117,   852,\n",
            "         2769,  6230,  2533,  8821,  1039,  4638,  7599,  3419,  3291,  1962,\n",
            "          671,   763,   117,  3300,   671,  2476,  1377,   809,  1288,  1309,\n",
            "         4638,  3763,  1355,   119,  2456,  6379,  1920,  2157,  6370,  3249,\n",
            "         6858,  7313,   106,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4w6AQZvgzce",
        "colab_type": "text"
      },
      "source": [
        "## 2.5 创建DataLoader，用来取出一个batch的数据\n",
        "\n",
        "TensorDataset 可以用来对 tensor 进行打包，就好像 python 中的 zip 功能。\n",
        "\n",
        "该类通过每一个 tensor 的第一个维度进行索引，所以该类中的 tensor 第一维度必须相等，且TensorDataset 中的参数必须是 tensor类型。\n",
        "\n",
        "RandomSampler对数据集随机采样。\n",
        "SequentialSampler按顺序对数据集采样。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBiQ9Bcg2RO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ1tYqiKnqGj",
        "colab_type": "text"
      },
      "source": [
        "查看一下train_dataloader的内容："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGWOL5ED2RI-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b14082c0-69f7-4f97-9074-bcdda3d34ab3"
      },
      "source": [
        "for i, (train, mask, label) in enumerate(train_dataloader): # torch.Size([16, 128]) torch.Size([16, 128]) torch.Size([16, 1])\n",
        "    print(train.shape, mask.shape, label.shape)\n",
        "    break\n",
        "\n",
        "print('len(train_dataloader) = ', len(train_dataloader))    # 500"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 128]) torch.Size([16, 128]) torch.Size([16, 1])\n",
            "len(train_dataloader) =  500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6PXyoEvofak",
        "colab_type": "text"
      },
      "source": [
        "# 3. 创建模型、优化器\n",
        "\n",
        "## 3.1 创建模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfcaltZp2RCg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1601dfa1-2f0b-41bb-c488-b19f050194b1"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels = 2) # num_labels表示2个分类,好评和差评\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6q1Bw6CqZk8",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 定义优化器\n",
        "\n",
        "参数eps是为了提高数值稳定性而添加到分母的一个项(默认: 1e-8)。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTVRhp7ipUfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr = learning_rate, eps = epsilon)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiOZPF6P4pMP",
        "colab_type": "text"
      },
      "source": [
        "更通用的写法：bias和LayNorm.weight没有用权重衰减"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lW1VZyWqzYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params' : [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                'weight_decay' : weight_decay},\n",
        "    {'params' : [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                'weight_decay' : 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr = learning_rate, eps = epsilon)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXe2LYE04Ri8",
        "colab_type": "text"
      },
      "source": [
        " ## 3.3 学习率预热，训练时先从小的学习率开始训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhYuopvX2yhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 2\n",
        "# training steps 的数量: [number of batches] x [number of epochs].\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# 设计 learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2-LLDoW6cZ7",
        "colab_type": "text"
      },
      "source": [
        "# 4.训练、评估模型 \n",
        "\n",
        "## 4.1 模型准确率\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFl32Ko724PI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_acc(preds, labels):\n",
        "    correct = torch.eq(torch.max(preds, dim=1)[1], labels.flatten()).float() # eq里面的两个参数的shape=torch.Size([16])\n",
        "    acc = correct.sum().item() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i32sHMh_GqR",
        "colab_type": "text"
      },
      "source": [
        "## 4.2 计算模型运行时间"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2CJDlV4-T2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round(elapsed))\n",
        "    return str(datetime.timedelta(seconds = elapsed_rounded)) # 返回 hh:mm:ss 形式的时间"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92O44C1-_0G-",
        "colab_type": "text"
      },
      "source": [
        "## 4.3 训练模型\n",
        "\n",
        "- 传入model的参数必须是tensor类型的；\n",
        "\n",
        "- nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=2)用于解决神经网络训练过拟合的方法 ；\n",
        "\n",
        "输入是（NN参数，最大梯度范数，范数类型=2) 一般默认为L2 范数；\n",
        "\n",
        "Tip： 注意这个方法只在训练的时候使用，在测试的时候不用；"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGUJfpT6_rjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer):\n",
        "    t0 = time.time()\n",
        "    avg_loss, avg_acc = [],[]\n",
        "\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # 每隔40个batch 输出一下所用时间.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids, b_input_mask, b_labels = batch[0].long().to(device), batch[1].long().to(device), batch[2].long().to(device)\n",
        "\n",
        "        output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss, logits = output[0], output[1]\n",
        "\n",
        "        avg_loss.append(loss.item())\n",
        "\n",
        "        acc = binary_acc(logits, b_labels)\n",
        "        avg_acc.append(acc)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), 1.0)      #大于1的梯度将其设为1.0, 以防梯度爆炸\n",
        "        optimizer.step()              #更新模型参数\n",
        "        scheduler.step()              #更新learning rate\n",
        "\n",
        "    avg_acc = np.array(avg_acc).mean()\n",
        "    avg_loss = np.array(avg_loss).mean()\n",
        "    return avg_loss, avg_acc"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGIZrUVhBVDR",
        "colab_type": "text"
      },
      "source": [
        "此处output的形式为（元组类型，第0个元素是loss值，第1个元素是每个batch中好评和差评的概率）："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDLQ9RobB9lt",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "(tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>), \n",
        "tensor([[-2.9815,  2.6931],\n",
        "        [-3.2380,  3.1935],\n",
        "        [-3.0775,  3.0713],\n",
        "        [ 3.0191, -2.3689],\n",
        "        [ 3.1146, -2.7957],\n",
        "        [ 3.7798, -2.7410],\n",
        "        [-0.3273,  0.8227],\n",
        "        [ 2.5012, -1.5535],\n",
        "        [-3.0231,  3.0162],\n",
        "        [ 3.4146, -2.5582],\n",
        "        [ 3.3104, -2.2134],\n",
        "        [ 3.3776, -2.5190],\n",
        "        [-2.6513,  2.5108],\n",
        "        [-3.3691,  2.9516],\n",
        "        [ 3.2397, -2.0473],\n",
        "        [-2.8622,  2.7395]], device='cuda:0', grad_fn=<AddmmBackward>))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW6qjSeBBfhk",
        "colab_type": "text"
      },
      "source": [
        "## 4.4 评估模型\n",
        "\n",
        "调用model模型时不传入label值。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5ELFI4bBQei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model):\n",
        "    avg_acc = []\n",
        "    model.eval()         #表示进入测试模式\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            b_input_ids, b_input_mask, b_labels = batch[0].long().to(device), batch[1].long().to(device), batch[2].long().to(device)\n",
        "\n",
        "            output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "            acc = binary_acc(output[0], b_labels)\n",
        "            avg_acc.append(acc)\n",
        "    avg_acc = np.array(avg_acc).mean()\n",
        "    return avg_acc"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rZvcWp4BytL",
        "colab_type": "text"
      },
      "source": [
        "此处output的形式为（元组类型，第0个元素是每个batch中好评和差评的概率）："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5ZiiaTjB0iu",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "(tensor([[ 3.8217, -2.7516],\n",
        "        [ 2.7585, -2.0853],\n",
        "        [-2.9317,  2.9092],\n",
        "        [-3.3724,  3.2597],\n",
        "        [-2.8692,  2.6741],\n",
        "        [-3.2784,  2.9276],\n",
        "        [ 3.4946, -2.8895],\n",
        "        [ 3.7855, -2.8623],\n",
        "        [-2.2249,  2.4336],\n",
        "        [-2.4257,  2.4606],\n",
        "        [ 3.3996, -2.5760],\n",
        "        [-3.1986,  3.0841],\n",
        "        [ 3.6883, -2.9492],\n",
        "        [ 3.2883, -2.3600],\n",
        "        [ 2.6723, -2.0778],\n",
        "        [-3.1868,  3.1106]], device='cuda:0'),)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EkPkI5eCHRo",
        "colab_type": "text"
      },
      "source": [
        "## 4.5 运行训练模型和评估模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jVbe00RBSLf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "cee6bed2-7f35-4b36-ca5e-9be3f0e1501c"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "\n",
        "    train_loss, train_acc = train(model, optimizer)\n",
        "    print('epoch={},训练准确率={}，损失={}'.format(epoch, train_acc, train_loss))\n",
        "    test_acc = evaluate(model)\n",
        "    print(\"epoch={},测试准确率={}\".format(epoch, test_acc))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Batch    40  of    500.    Elapsed: 0:00:27.\n",
            "  Batch    80  of    500.    Elapsed: 0:00:53.\n",
            "  Batch   120  of    500.    Elapsed: 0:01:20.\n",
            "  Batch   160  of    500.    Elapsed: 0:01:47.\n",
            "  Batch   200  of    500.    Elapsed: 0:02:14.\n",
            "  Batch   240  of    500.    Elapsed: 0:02:40.\n",
            "  Batch   280  of    500.    Elapsed: 0:03:07.\n",
            "  Batch   320  of    500.    Elapsed: 0:03:34.\n",
            "  Batch   360  of    500.    Elapsed: 0:04:01.\n",
            "  Batch   400  of    500.    Elapsed: 0:04:28.\n",
            "  Batch   440  of    500.    Elapsed: 0:04:55.\n",
            "  Batch   480  of    500.    Elapsed: 0:05:22.\n",
            "epoch=0,训练准确率=0.90275，损失=0.2619755164962262\n",
            "epoch=0,测试准确率=0.9325\n",
            "  Batch    40  of    500.    Elapsed: 0:00:27.\n",
            "  Batch    80  of    500.    Elapsed: 0:00:53.\n",
            "  Batch   120  of    500.    Elapsed: 0:01:20.\n",
            "  Batch   160  of    500.    Elapsed: 0:01:47.\n",
            "  Batch   200  of    500.    Elapsed: 0:02:14.\n",
            "  Batch   240  of    500.    Elapsed: 0:02:41.\n",
            "  Batch   280  of    500.    Elapsed: 0:03:08.\n",
            "  Batch   320  of    500.    Elapsed: 0:03:35.\n",
            "  Batch   360  of    500.    Elapsed: 0:04:02.\n",
            "  Batch   400  of    500.    Elapsed: 0:04:28.\n",
            "  Batch   440  of    500.    Elapsed: 0:04:55.\n",
            "  Batch   480  of    500.    Elapsed: 0:05:22.\n",
            "epoch=1,训练准确率=0.953375，损失=0.15345162890665234\n",
            "epoch=1,测试准确率=0.9435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QchQuTWDCM6M",
        "colab_type": "text"
      },
      "source": [
        "# 5. 预测"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpM5amxqCKbk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "ff51d8f9-7797-480a-d326-f54c1c0eebc9"
      },
      "source": [
        "def predict(sen):\n",
        "\n",
        "    input_id = convert_text_to_token(tokenizer, sen)\n",
        "    input_token =  torch.tensor(input_id).long().to(device)            #torch.Size([128])\n",
        "\n",
        "    atten_mask = [float(i>0) for i in input_id]\n",
        "    attention_token = torch.tensor(atten_mask).long().to(device)       #torch.Size([128])\n",
        "\n",
        "    output = model(input_token.view(1, -1), token_type_ids=None, attention_mask=attention_token.view(1, -1))     #torch.Size([128])->torch.Size([1, 128])否则会报错\n",
        "    print(output[0])\n",
        "\n",
        "    return torch.max(output[0], dim=1)[1]\n",
        "\n",
        "label = predict('酒店位置难找，环境不太好，隔音差，下次不会再来的。')\n",
        "print('好评' if label==1 else '差评')\n",
        "\n",
        "label = predict('酒店还可以，接待人员很热情，卫生合格，空间也比较大，不足的地方就是没有窗户')\n",
        "print('好评' if label==1 else '差评')\n",
        "\n",
        "label = predict('\"服务各方面没有不周到的地方, 各方面没有没想到的细节\"')\n",
        "print('好评' if label==1 else '差评')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 2.3040, -4.0122]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "差评\n",
            "tensor([[-1.5570,  2.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "好评\n",
            "tensor([[ 0.3791, -1.3262]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "差评\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsTJVQSrD9XR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}