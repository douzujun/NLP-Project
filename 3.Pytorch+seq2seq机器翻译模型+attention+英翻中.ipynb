{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "3.Pytorch+seq2seq机器翻译模型+attention+英翻中.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XHpdbe4NcoNl"
      },
      "source": [
        "# Seq2Seq + Attention\n",
        "\n",
        "在这份notebook当中，尽可能复现Luong的attention模型\n",
        "\n",
        "数据集非常小，只有一万多个句子的训练数据，所以训练出来的模型效果并不好。如果想训练一个好一点的模型，可以参考下面的资料。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5RqjiiwJcoNm"
      },
      "source": [
        "## 更多阅读\n",
        "\n",
        "#### 课件\n",
        "- [cs224d](http://cs224d.stanford.edu/lectures/CS224d-Lecture15.pdf)\n",
        "\n",
        "\n",
        "#### 论文\n",
        "- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
        "- [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025?context=cs)\n",
        "- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1406.1078)\n",
        "\n",
        "\n",
        "#### PyTorch代码\n",
        "- [seq2seq-tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb)\n",
        "- [Tutorial from Ben Trevett](https://github.com/bentrevett/pytorch-seq2seq)\n",
        "- [IBM seq2seq](https://github.com/IBM/pytorch-seq2seq)\n",
        "- [OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py) 较好\n",
        "\n",
        "\n",
        "#### 更多关于Machine Translation\n",
        "- [Beam Search](https://www.coursera.org/lecture/nlp-sequence-models/beam-search-4EtHZ)\n",
        "- Pointer network 文本摘要\n",
        "- Copy Mechanism 文本摘要\n",
        "- Converage Loss \n",
        "- ConvSeq2Seq\n",
        "- Transformer\n",
        "- Tensor2Tensor\n",
        "\n",
        "#### TODO\n",
        "- 尝试对中文进行分词\n",
        "\n",
        "#### NER\n",
        "- https://github.com/allenai/allennlp/tree/master/allennlp\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QTPmv1ERcoNn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b11dc1fe-1c8a-41c1-9cf9-478ced3da75a"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "from collections import Counter \n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ON3mZkKWcoNq"
      },
      "source": [
        "读入中英文数据\n",
        "- 英文使用nltk的word tokenizer来分词，并且使用小写字母\n",
        "- 中文使用单个汉字作为基本单元"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uMyTGTsPcoNr",
        "colab": {}
      },
      "source": [
        "def load_data(in_file):\n",
        "    cn = []\n",
        "    en = []\n",
        "    num_examples = 0\n",
        "    with open(in_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip().split(\"\\t\") \n",
        "            en.append([\"BOS\"] + nltk.word_tokenize(line[0].lower()) + [\"EOS\"])           \n",
        "            cn.append([\"BOS\"] + [c for c in line[1]] + [\"EOS\"])\n",
        "    return en, cn\n",
        " \n",
        "train_file = \"./sample_data/train.txt\"\n",
        "dev_file = \"./sample_data/dev.txt\"\n",
        "train_en, train_cn = load_data(train_file)\n",
        "dev_en, dev_cn = load_data(dev_file)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Yd2oM5OgmuQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "1ff701de-af5a-4104-bded-78deed959db9"
      },
      "source": [
        "print(train_en[:10])\n",
        "print(train_cn[:10])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['BOS', 'anyone', 'can', 'do', 'that', '.', 'EOS'], ['BOS', 'how', 'about', 'another', 'piece', 'of', 'cake', '?', 'EOS'], ['BOS', 'she', 'married', 'him', '.', 'EOS'], ['BOS', 'i', 'do', \"n't\", 'like', 'learning', 'irregular', 'verbs', '.', 'EOS'], ['BOS', 'it', \"'s\", 'a', 'whole', 'new', 'ball', 'game', 'for', 'me', '.', 'EOS'], ['BOS', 'he', \"'s\", 'sleeping', 'like', 'a', 'baby', '.', 'EOS'], ['BOS', 'he', 'can', 'play', 'both', 'tennis', 'and', 'baseball', '.', 'EOS'], ['BOS', 'we', 'should', 'cancel', 'the', 'hike', '.', 'EOS'], ['BOS', 'he', 'is', 'good', 'at', 'dealing', 'with', 'children', '.', 'EOS'], ['BOS', 'she', 'will', 'do', 'her', 'best', 'to', 'be', 'here', 'on', 'time', '.', 'EOS']]\n",
            "[['BOS', '任', '何', '人', '都', '可', '以', '做', '到', '。', 'EOS'], ['BOS', '要', '不', '要', '再', '來', '一', '塊', '蛋', '糕', '？', 'EOS'], ['BOS', '她', '嫁', '给', '了', '他', '。', 'EOS'], ['BOS', '我', '不', '喜', '欢', '学', '习', '不', '规', '则', '动', '词', '。', 'EOS'], ['BOS', '這', '對', '我', '來', '說', '是', '個', '全', '新', '的', '球', '類', '遊', '戲', '。', 'EOS'], ['BOS', '他', '正', '睡', '着', '，', '像', '个', '婴', '儿', '一', '样', '。', 'EOS'], ['BOS', '他', '既', '会', '打', '网', '球', '，', '又', '会', '打', '棒', '球', '。', 'EOS'], ['BOS', '我', '們', '應', '該', '取', '消', '這', '次', '遠', '足', '。', 'EOS'], ['BOS', '他', '擅', '長', '應', '付', '小', '孩', '子', '。', 'EOS'], ['BOS', '她', '会', '尽', '量', '按', '时', '赶', '来', '的', '。', 'EOS']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AneLf_O8coNv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "61b920a6-d06b-4f0c-b98a-38ef20070f15"
      },
      "source": [
        "UNK_IDX = 0\n",
        "PAD_IDX = 1\n",
        "def build_dict(sentences, max_words=50000):\n",
        "    word_count = Counter() \n",
        "    for sentence in sentences:\n",
        "        for s in sentence:\n",
        "            word_count[s] += 1  \n",
        "    ls = word_count.most_common(max_words) \n",
        "    print(len(ls)) #train_en：5491，train_cn=3193\n",
        "    \n",
        "    total_words = len(ls) + 2   \n",
        "    word_dict = {w[0]: index+2 for index, w in enumerate(ls)}\n",
        "    \n",
        "    word_dict[\"UNK\"] = UNK_IDX \n",
        "    word_dict[\"PAD\"] = PAD_IDX \n",
        "    return word_dict, total_words\n",
        "\n",
        "en_dict, en_total_words = build_dict(train_en) \n",
        "cn_dict, cn_total_words = build_dict(train_cn)\n",
        "\n",
        "inv_en_dict = {v: k for k, v in en_dict.items()}\n",
        "inv_cn_dict = {v: k for k, v in cn_dict.items()}"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5491\n",
            "3193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2GyNIK_skLlS",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "e816a589-2839-4253-c59f-9080e46f03f0"
      },
      "source": [
        "print(en_total_words)\n",
        "print(list(en_dict.items())[:10]) # 取出前10个\n",
        "print(list(en_dict.items())[-10:]) # 取出后10个，可以看到\"unk\"和\"pad\"在最后\n",
        "print(\"---\"*20)\n",
        "print(cn_total_words)\n",
        "print(list(cn_dict.items())[:10]) # 查看中文\n",
        "print(list(cn_dict.items())[-10:]) \n",
        "print(\"---\"*20)\n",
        "print(list(inv_en_dict.items())[:10]) # 键值对调换\n",
        "print(list(inv_cn_dict.items())[:10]) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5493\n",
            "[('BOS', 2), ('EOS', 3), ('.', 4), ('i', 5), ('the', 6), ('to', 7), ('you', 8), ('a', 9), ('is', 10), ('?', 11)]\n",
            "[('opposition', 5485), ('springs', 5486), ('schoolroom', 5487), ('absence', 5488), ('fonder', 5489), ('field', 5490), ('educational', 5491), ('foster', 5492), ('UNK', 0), ('PAD', 1)]\n",
            "------------------------------------------------------------\n",
            "3195\n",
            "[('BOS', 2), ('EOS', 3), ('。', 4), ('我', 5), ('的', 6), ('了', 7), ('你', 8), ('他', 9), ('是', 10), ('一', 11)]\n",
            "[('鷹', 3187), ('鸚', 3188), ('鵡', 3189), ('寵', 3190), ('鳴', 3191), ('缓', 3192), ('黨', 3193), ('釘', 3194), ('UNK', 0), ('PAD', 1)]\n",
            "------------------------------------------------------------\n",
            "[(2, 'BOS'), (3, 'EOS'), (4, '.'), (5, 'i'), (6, 'the'), (7, 'to'), (8, 'you'), (9, 'a'), (10, 'is'), (11, '?')]\n",
            "[(2, 'BOS'), (3, 'EOS'), (4, '。'), (5, '我'), (6, '的'), (7, '了'), (8, '你'), (9, '他'), (10, '是'), (11, '一')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SJWu3OAGcoNy",
        "colab": {}
      },
      "source": [
        "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):\n",
        "  \n",
        "    length = len(en_sentences)\n",
        "    # en_dict.get(w, 0)，返回键w对应的值,没有为0\n",
        "    out_en_sentences = [[en_dict.get(w, 0) for w in sent] for sent in en_sentences] \n",
        "    out_cn_sentences = [[cn_dict.get(w, 0) for w in sent] for sent in cn_sentences]\n",
        "\n",
        "    def len_argsort(seq):\n",
        "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
        "        # 按seq[x]的长度排序，最短句子的索引排在最前面\n",
        "    \n",
        "    # sort sentences by english lengths\n",
        "    if sort_by_len:\n",
        "        sorted_index = len_argsort(out_en_sentences)\n",
        "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]    \n",
        "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
        "    \n",
        "    return out_en_sentences, out_cn_sentences\n",
        "\n",
        "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
        "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFqBAYwyWmLH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2e2f89fc-5d31-4259-c4d6-68e2bed72bb5"
      },
      "source": [
        "# sorted示例\n",
        "seq = [5,4,6,9,10]\n",
        "print(sorted(range(5), key=lambda x: seq[x])) \n",
        "print(sorted(range(4), key=lambda x: seq[x]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 0, 2, 3, 4]\n",
            "[1, 0, 2, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SudDvV1WmLU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "468dee67-186c-4de1-b6d7-2c2e33988432"
      },
      "source": [
        "print(train_en[:10])\n",
        "print(train_cn[:10])\n",
        "print(\"---\"*20)\n",
        "k=10000 # \n",
        "print([inv_cn_dict[i] for i in train_cn[k]]) \n",
        "print([inv_en_dict[i] for i in train_en[k]])\n",
        "print(\" \".join([inv_cn_dict[i] for i in train_cn[k]])) \n",
        "print(\" \".join([inv_en_dict[i] for i in train_en[k]])) "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 475, 4, 3], [2, 1318, 126, 3], [2, 1707, 126, 3], [2, 254, 126, 3], [2, 1318, 126, 3], [2, 130, 11, 3], [2, 2045, 126, 3], [2, 693, 126, 3], [2, 2266, 126, 3], [2, 1707, 126, 3]]\n",
            "[[2, 8, 87, 441, 6, 4, 3], [2, 119, 1368, 221, 3], [2, 982, 2028, 8, 4, 3], [2, 239, 239, 221, 3], [2, 151, 190, 221, 3], [2, 8, 546, 162, 14, 3], [2, 141, 488, 6, 221, 3], [2, 18, 489, 221, 3], [2, 189, 158, 221, 3], [2, 2110, 60, 221, 3]]\n",
            "------------------------------------------------------------\n",
            "['BOS', '他', '来', '这', '里', '的', '目', '的', '是', '什', '么', '？', 'EOS']\n",
            "['BOS', 'for', 'what', 'purpose', 'did', 'he', 'come', 'here', '?', 'EOS']\n",
            "BOS 他 来 这 里 的 目 的 是 什 么 ？ EOS\n",
            "BOS for what purpose did he come here ? EOS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Ugb6nb1Zn8X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "72d92555-cf3d-42b7-d3d0-91989c590680"
      },
      "source": [
        "print(np.arange(0, 100, 15))\n",
        "print(np.arange(0, 15))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0 15 30 45 60 75 90]\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xP9XSbJHcoN6",
        "colab": {}
      },
      "source": [
        "# 按句子的数量自制batch\n",
        "def get_batches(n, batch_size, shuffle=True):\n",
        "    idx_list = np.arange(0, n, batch_size) \n",
        "    if shuffle:\n",
        "        np.random.shuffle(idx_list) #打乱数据\n",
        "    batches = []\n",
        "    for idx in idx_list:\n",
        "        batches.append(np.arange(idx, min(idx + batch_size, n)))\n",
        "        # 所有batch放在一个大列表里\n",
        "    return batches"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cLHziG0aZtwr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "44ecc372-ba94-41e1-84ef-a237669635b6"
      },
      "source": [
        "get_batches(100,15) #随机打乱的"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]),\n",
              " array([15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n",
              " array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]),\n",
              " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),\n",
              " array([45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]),\n",
              " array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
              " array([75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bzMTd27YZqcf",
        "colab": {}
      },
      "source": [
        "# 对句子做padding\n",
        "def sent_padding(seqs):\n",
        "    lengths = [len(seq) for seq in seqs]\n",
        "    n_samples = len(seqs) \n",
        "    max_len = np.max(lengths) # 取出最长的的语句长度\n",
        "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
        "    x_lengths = np.array(lengths).astype(\"int32\")\n",
        "    \n",
        "    for idx, seq in enumerate(seqs):\n",
        "        x[idx, :lengths[idx]] = seq\n",
        "    \n",
        "    # x: padding后的句子\n",
        "    # x_lengths:每句话的length\n",
        "    return x, x_lengths \n",
        "\n",
        "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
        "    batches = get_batches(len(en_sentences), batch_size)\n",
        "    all_ex = []\n",
        "    for batch in batches: \n",
        "        mb_en_sentences = [en_sentences[t] for t in batch]        \n",
        "        mb_cn_sentences = [cn_sentences[t] for t in batch]\n",
        "        # padding\n",
        "        mb_x, mb_x_len = sent_padding(mb_en_sentences)\n",
        "        mb_y, mb_y_len = sent_padding(mb_cn_sentences)\n",
        "        \n",
        "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
        "        # （英文句子，英文句子长度，中文句子，中文句子长度） \n",
        "    return all_ex\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "train_data = gen_examples(train_en, train_cn, batch_size)  # (mb_x, mb_x_len, mb_y, mb_y_len)\n",
        "random.shuffle(train_data) \n",
        "dev_data = gen_examples(dev_en, dev_cn, batch_size) "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wBE0PAtSL_iC",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04e31885-63a2-4585-f13a-c469a12f81b8"
      },
      "source": [
        "# 打印第一个batch的信息\n",
        "print(train_data[0][0].shape) # 一个batch英文句子维度\n",
        "print(train_data[0][1].shape) # 一个batch英文句子长度维度\n",
        "print(train_data[0][2].shape) # 一个batch中文句子维度\n",
        "print(train_data[0][3].shape) # 一个batch中文句子长度维度\n",
        "print(train_data[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 8)\n",
            "(64,)\n",
            "(64, 16)\n",
            "(64,)\n",
            "(array([[   2,    5,   22,  279,  274,  295,    4,    3],\n",
            "       [   2,    6,  991, 3456,    6,  152,    4,    3],\n",
            "       [   2,    5,   79,  111,  274,  399,    4,    3],\n",
            "       [   2,    5,  120,  181,   21, 1321,    4,    3],\n",
            "       [   2,    5,  855,    8,   30,  146,    4,    3],\n",
            "       [   2,   30,    8,   92,    7,  565,   11,    3],\n",
            "       [   2,    5,   56,  181,  246,   16,    4,    3],\n",
            "       [   2,   18,  227,   23,    6,  412,    4,    3],\n",
            "       [   2,  244,   10,   32,  640,  765,   11,    3],\n",
            "       [   2,  137,   23,   31,    7,   14,    4,    3],\n",
            "       [   2,   31,   20,   21,  131,  580,   11,    3],\n",
            "       [   2,   18,   10,   13,    9, 1339,    4,    3],\n",
            "       [   2, 3471,  197,   45, 1463, 1709,    4,    3],\n",
            "       [   2,   88,   37,   95,   24,   80,   11,    3],\n",
            "       [   2,    6, 2060,   10,   13,  357,    4,    3],\n",
            "       [   2,   30,    8,  259,   34,  107,   11,    3],\n",
            "       [   2,  108,   47,   39,  984, 1490,   11,    3],\n",
            "       [   2,    5,   14,   13,   36,  497,    4,    3],\n",
            "       [   2,  424,  227,    7, 3481,  172,    4,    3],\n",
            "       [   2,    8,  258,  128,  125,  405,    4,    3],\n",
            "       [   2,   18,   48,    9,  221, 1653,    4,    3],\n",
            "       [   2,   43,    5,  280,   32,  589,   11,    3],\n",
            "       [   2,    5,   27, 2066,   24,  128,    4,    3],\n",
            "       [   2,   12, 1209,  201,    6,  417,    4,    3],\n",
            "       [   2,    5,   36,  968,   33, 3487,    4,    3],\n",
            "       [   2,   12,   27,  569,   15, 1338,    4,    3],\n",
            "       [   2,   14,   13,  684,   32,  127,    4,    3],\n",
            "       [   2,    5,  780,   46,  303,  459,    4,    3],\n",
            "       [   2,   25,   10,  111,    9, 3501,    4,    3],\n",
            "       [   2,    5,   42,   33,    9, 2069,    4,    3],\n",
            "       [   2,   28,   94,   20,  175,   59,    4,    3],\n",
            "       [   2,   31,   70,  424,  526,   81,   11,    3],\n",
            "       [   2,    6,  214,  628,    7, 1165,    4,    3],\n",
            "       [   2,    8,   30,  118,  192,  315,    4,    3],\n",
            "       [   2,   12,  479,    7,   35,  326,    4,    3],\n",
            "       [   2,   18,  628,   57,    6,  346,    4,    3],\n",
            "       [   2,   18,   27,  176,   26,  295,    4,    3],\n",
            "       [   2,    5,   76,  721,    9,  177,    4,    3],\n",
            "       [   2,   12,   27, 1347,   17, 1718,    4,    3],\n",
            "       [   2,   25,  943,   10,   13,  381,    4,    3],\n",
            "       [   2,   19,   27, 2073,   64,  578,    4,    3],\n",
            "       [   2,   19,  144,   23,  562,  248,    4,    3],\n",
            "       [   2,    8,  820,    7,   53,  317,    4,    3],\n",
            "       [   2,    5,  283,    8, 3510,   18,    4,    3],\n",
            "       [   2,   47,  353,  278,  970,    8,   11,    3],\n",
            "       [   2,    8,   79,   84,    7, 1057,    4,    3],\n",
            "       [   2,    5,   81,   18,   10, 3515,    4,    3],\n",
            "       [   2,   12, 1340,   26,    6, 2566,    4,    3],\n",
            "       [   2,   12,   93,   64,  377, 2567,    4,    3],\n",
            "       [   2,  183,   74,   17,    6,  255,    4,    3],\n",
            "       [   2,  157,   67,  955,  188,  326,    4,    3],\n",
            "       [   2,   14,    8,   53,   21,  232,   11,    3],\n",
            "       [   2,  344,   30,   37, 1062, 3522,    4,    3],\n",
            "       [   2,   72,   33,   24,  293,  218,    4,    3],\n",
            "       [   2,    6,  340,   10,   61, 1645,    4,    3],\n",
            "       [   2,   31, 2572,   30,    8,  898,   11,    3],\n",
            "       [   2,   18,   10,    9, 2573, 2078,    4,    3],\n",
            "       [   2,   18,   48,  374, 1198, 1091,    4,    3],\n",
            "       [   2,   16,   10,   64, 3525, 3526,    4,    3],\n",
            "       [   2,   43,  304,  285,    6,  524,   11,    3],\n",
            "       [   2,    5,   14,   13, 1017,    8,    4,    3],\n",
            "       [   2,   18,  958,   41,  150,  146,    4,    3],\n",
            "       [   2,    5,  123,    7, 3529,  119,    4,    3],\n",
            "       [   2,   28,  352, 1313,   21,  310,    4,    3]], dtype=int32), array([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8],\n",
            "      dtype=int32), array([[  2,   5, 154, ...,   0,   0,   0],\n",
            "       [  2,  36, 176, ...,   0,   0,   0],\n",
            "       [  2,   5, 705, ...,   0,   0,   0],\n",
            "       ...,\n",
            "       [  2,  29,  19, ...,   3,   0,   0],\n",
            "       [  2, 131,  23, ...,   0,   0,   0],\n",
            "       [  2,  28, 264, ...,   0,   0,   0]], dtype=int32), array([10, 12, 10, 12,  9,  8, 10, 11, 10, 10, 11,  9, 12, 16, 10, 10, 10,\n",
            "        9, 11,  8,  9, 12,  7, 10, 14,  9, 10, 11,  9,  7,  9,  9,  8, 12,\n",
            "       11, 11, 10, 10, 10,  8,  9, 10, 12, 13, 10,  8, 11, 10, 13,  5, 14,\n",
            "       11, 11,  9, 12, 12, 11,  9, 14, 11,  7, 14, 13, 11], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z_ncYOpDcoN-"
      },
      "source": [
        "### 没有Attention的Encoder-Decoder版本"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fBf8n8o8coN_",
        "colab": {}
      },
      "source": [
        "class PlainEncoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
        "        #以英文为例，vocab_size=5493, hidden_size=100, dropout=0.2\n",
        "        super(PlainEncoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)      \n",
        "        #第一个参数为input_size： embedding_dim\n",
        "        #第二个参数为hidden_size：隐藏层维度\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, lengths): \n",
        "        # pack,padded的操作需要句子排序，降序排列\n",
        "        sorted_len, sorted_idx = lengths.sort(0, descending=True)  \n",
        "        x_sorted = x[sorted_idx.long()]      \n",
        "        embedded = self.dropout(self.embed(x_sorted))   # embedded：[64, 10, 100]\n",
        "        \n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
        "        packed_out, hid = self.rnn(packed_embedded)     # hid: [1, 64, 100]\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
        "        # out: [64, 10, 100]\n",
        "\n",
        "        _, original_idx = sorted_idx.sort(0, descending=False)  \n",
        "        # 上面lengths.sort()过，已经打乱了batch的句子的顺序，所以得恢复原位置，不然跟中文对不上\n",
        "        out = out[original_idx.long()].contiguous()      # out:[64, 10, 100]\n",
        "        hid = hid[:, original_idx.long()].contiguous()   # hid:[1, 64, 100], 在batch的维度上进行排序还原\n",
        "  \n",
        "        return out, hid[[-1]]  # hid取出最后一层"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z0tjDSNYWmMX",
        "colab": {}
      },
      "source": [
        "class PlainDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
        "        super(PlainDecoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size) \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, y, y_lengths, hid):\n",
        "        # y: [64, 12]\n",
        "        # hid: [1, 64, 100]\n",
        "        # 中文的y和y_lengths\n",
        "        \n",
        "        # 中文句子的长度也不一样，也要和上面一样，不同长度的句子，也应该去掉没用的神经元\n",
        "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
        "        y_sorted = y[sorted_idx.long()]\n",
        "        hid = hid[:, sorted_idx.long()] #隐藏层也要排序\n",
        "        # hid是Encoder的输出，和y_sorted都作为输入进入decoder层\n",
        "        y_sorted = self.dropout(self.embed(y_sorted)) \n",
        "        # batch_size, output_length, embed_size\n",
        "        \n",
        "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
        "        out, hid = self.rnn(packed_seq, hid)  # hid：[1, 64, 100], 默认传入0向量\n",
        "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
        "        \n",
        "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
        "        output_seq = unpacked[original_idx.long()].contiguous()\n",
        "        # output_seq：[64, 12, 100]\n",
        "        hid = hid[:, original_idx.long()].contiguous()\n",
        "        # hid：[1, 64, 100]\n",
        "        \n",
        "        output = F.log_softmax(self.out(output_seq), -1)\n",
        "        # output：[64, 12, 3195]\n",
        "        \n",
        "        return output, hid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OwlWt-aWmMe",
        "colab_type": "code",
        "colab": {},
        "outputId": "5db6620c-9569-49f0-bcfc-be3e292137ee"
      },
      "source": [
        "lengths = torch.tensor([10,4,6,3,7])\n",
        "a, b = lengths.sort(0, descending=True)\n",
        "a, b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([10,  7,  6,  4,  3]), tensor([0, 4, 2, 1, 3]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Jsbd07kWmMr",
        "colab": {}
      },
      "source": [
        "class PlainSeq2Seq(nn.Module):\n",
        "    \n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(PlainSeq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "    def forward(self, x, x_lengths, y, y_lengths):\n",
        "        encoder_out, hid = self.encoder(x, x_lengths)\n",
        "        output, hid = self.decoder(y, y_lengths, hid)      \n",
        "        return output, None\n",
        "\n",
        "    def translate(self, x, x_lengths, y, max_length=10):      \n",
        "        encoder_out, hid = self.encoder(x, x_lengths)\n",
        "        # encoder_out.shape=torch.Size([1, 7, 100])，1是batch_size,7是句子长度\n",
        "        # hid.shape=torch.Size([1, 1, 100])，\n",
        "        preds = []\n",
        "        batch_size = x.shape[0]\n",
        "        attns = []\n",
        "        for i in range(max_length):\n",
        "            # 训练的时候y是一个句子，一起decoder训练\n",
        "            # 测试的时候y是个一个词一个词生成的，所以这里的y是传入的第一个单词，这里是bos\n",
        "            # 同理y_lengths也是1\n",
        "            output, hid = self.decoder(y=y,\n",
        "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
        "                    hid=hid)         \n",
        "            #刚开始循环bos作为模型的首个输入单词，后续更新y，下个预测单词的输入是上个输出单词\n",
        "            # output.shape = torch.Size([1, 1, 3195])\n",
        "            # hid.shape = torch.Size([1, 1, 100])\n",
        "\n",
        "            y = output.max(2)[1].view(batch_size, 1)\n",
        "            # .max(2)在第三个维度上取最大值,返回最大值和对应的位置索引，[1]取出最大值所在的索引\n",
        "            preds.append(y) # 每次循环输出的y值就是预测值\n",
        "            # preds = [tensor([[5]], device='cuda:0'), tensor([[24]], device='cuda:0'), ... tensor([[4]], device='cuda:0')]\n",
        "            # torch.cat(preds, 1) = tensor([[ 5, 24,  6, 22,  7,  4,  3,  4,  3,  4]], device='cuda:0')\n",
        "        return torch.cat(preds, 1), None  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDDUrPf5WmM0",
        "colab_type": "code",
        "colab": {},
        "outputId": "3255ef80-250f-48e4-ab6f-34fcd2fac09a"
      },
      "source": [
        "a = torch.rand([1, 2, 5])\n",
        "print(a)\n",
        "print(a.max(2))\n",
        "a.max(2)[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.7248, 0.3954, 0.5828, 0.2874, 0.3311],\n",
            "         [0.5365, 0.3869, 0.6046, 0.9320, 0.7759]]])\n",
            "torch.return_types.max(\n",
            "values=tensor([[0.7248, 0.9320]]),\n",
            "indices=tensor([[0, 3]]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vGykJm8ucoOE",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dropout = 0.2\n",
        "hidden_size = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QTkllA2xWmNC",
        "colab": {}
      },
      "source": [
        "# 实例model\n",
        "encoder = PlainEncoder(vocab_size=en_total_words,\n",
        "                      hidden_size=hidden_size,\n",
        "                      dropout=dropout)\n",
        "decoder = PlainDecoder(vocab_size=cn_total_words,\n",
        "                      hidden_size=hidden_size,\n",
        "                      dropout=dropout)\n",
        "model = PlainSeq2Seq(encoder, decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kF-lEtsVcoOB",
        "colab": {}
      },
      "source": [
        "# masked cross entropy loss\n",
        "class LanguageModelCriterion(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LanguageModelCriterion, self).__init__()\n",
        "\n",
        "    def forward(self, input, target, mask):\n",
        "        # input: [64, 12, 3195] target: [64, 12]  mask: [64, 12]\n",
        "        # input: (batch_size * seq_len) * vocab_size\n",
        "        input = input.contiguous().view(-1, input.size(2))\n",
        "        # target: batch_size * seq_len\n",
        "        target = target.contiguous().view(-1, 1)\n",
        "        mask = mask.contiguous().view(-1, 1)\n",
        "        output = -input.gather(1, target) * mask  # 将input在1维，把target当索引进行取值\n",
        "        #这里算得就是交叉熵损失，前面已经算了F.log_softmax\n",
        "        #output.shape=torch.Size([768, 1])\n",
        "        #因为input.gather时，target为0的地方不是零了，mask作用是把padding为0的地方重置为零，\n",
        "        #因为在volab里0代表的也是一个单词，但是我们这里target尾部的0代表的不是单词\n",
        "        output = torch.sum(output) / torch.sum(mask)\n",
        "        # 均值损失，output前已经加了负号，所以这里还是最小化\n",
        "        return output"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AgMj5SDIWmNP",
        "colab": {}
      },
      "source": [
        "model = model.to(device)\n",
        "loss_fn = LanguageModelCriterion().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nii8-NKpcoOG",
        "colab": {}
      },
      "source": [
        "def evaluate(model, data):\n",
        "    model.eval()\n",
        "    total_num_words = total_loss = 0.\n",
        "    with torch.no_grad():#不需要更新模型，不需要梯度\n",
        "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
        "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
        "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
        "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
        "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
        "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
        "            mb_y_len[mb_y_len<=0] = 1\n",
        "\n",
        "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
        "\n",
        "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
        "            mb_out_mask = mb_out_mask.float()\n",
        "\n",
        "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
        "\n",
        "            num_words = torch.sum(mb_y_len).item()\n",
        "            total_loss += loss.item() * num_words\n",
        "            total_num_words += num_words\n",
        "    print(\"Evaluation loss\", total_loss/total_num_words)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bZdgC77scoOI",
        "colab": {}
      },
      "source": [
        "def train(model, data, num_epochs=2):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_num_words = total_loss = 0.\n",
        "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
        "            #（英文batch，英文长度，中文batch，中文长度）         \n",
        "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
        "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()  \n",
        "            # 前n-1个单词作为输入，后n-1个单词作为输出，因为输入的前一个单词要预测后一个单词\n",
        "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
        "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
        "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
        "           \n",
        "            mb_y_len[mb_y_len<=0] = 1\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
        "            \n",
        "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
        "            # None：在这个位置上增加一个维度\n",
        "            #  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "            #         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "            #         [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "            #         [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
        "            #         [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
        "            #         [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
        "            #         [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
        "            #         [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
        "            #         [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
        "            #         [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]])\n",
        "            mb_out_mask = mb_out_mask.float()  # 下三角矩阵\n",
        "\n",
        "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
        "            \n",
        "            num_words = torch.sum(mb_y_len).item()  # 一个batch里多少个单词 \n",
        "            total_loss += loss.item() * num_words \n",
        "            total_num_words += num_words\n",
        "          \n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
        "            #为了防止梯度过大，设置梯度的阈值 \n",
        "            optimizer.step()\n",
        "            \n",
        "            if it % 100 == 0:\n",
        "                print(\"Epoch\", epoch, \"iteration\", it, \"loss\", loss.item())\n",
        "\n",
        "        print(\"Epoch\", epoch, \"Training loss\", total_loss/total_num_words)\n",
        "        if epoch % 5 == 0:\n",
        "            evaluate(model, dev_data) \n",
        "            \n",
        "# train(model, train_data, num_epochs=20)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dvf_KTw8coOL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e7672930-6075-47a1-b552-962d728c92b6"
      },
      "source": [
        "# 翻译个句子试试\n",
        "def translate_dev(i):\n",
        "    en_sent = \" \".join([inv_en_dict[w] for w in dev_en[i]])\n",
        "    print(en_sent)\n",
        "    cn_sent = \" \".join([inv_cn_dict[w] for w in dev_cn[i]])\n",
        "    print(\"\".join(cn_sent))\n",
        "\n",
        "    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(1, -1)).long().to(device)   \n",
        "    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)\n",
        "    bos = torch.Tensor([[cn_dict[\"BOS\"]]]).long().to(device)\n",
        "\n",
        "    translation, attn = model.translate(mb_x, mb_x_len, bos)\n",
        "    # 这里传入bos作为首个单词的输入\n",
        "    #translation=tensor([[ 8,  6, 11, 25, 22, 57, 10,  5,  6,  4]], device='cuda:0')\n",
        "    translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
        "     \n",
        "    trans = []\n",
        "    for word in translation:\n",
        "        if word != \"EOS\": \n",
        "            trans.append(word) \n",
        "        else:\n",
        "            break\n",
        "    print(\"\".join(trans))\n",
        "    \n",
        "for i in range(500,520):\n",
        "    translate_dev(i)\n",
        "    print()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOS let me have a look . EOS\n",
            "BOS 讓 我 看 看 。 EOS\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "我我我我我我。\n",
            "\n",
            "BOS he took off his glasses . EOS\n",
            "BOS 他 摘 下 了 眼 鏡 。 EOS\n",
            "我我我我我我我我。\n",
            "\n",
            "BOS tom is very much alone . EOS\n",
            "BOS 汤 姆 非 常 孤 单 。 EOS\n",
            "我我我我我我我。\n",
            "\n",
            "BOS tom is a good person . EOS\n",
            "BOS 湯 姆 是 個 好 人 。 EOS\n",
            "我我我我我。\n",
            "\n",
            "BOS it 's already nine o'clock . EOS\n",
            "BOS 已 经 9 点 了 。 EOS\n",
            "我我我我我我。\n",
            "\n",
            "BOS i 've quit drinking beer . EOS\n",
            "BOS 我 已 經 不 喝 啤 酒 了 。 EOS\n",
            "我我我我我我我。\n",
            "\n",
            "BOS my mother boiled ten eggs . EOS\n",
            "BOS 妈 妈 煮 了 十 只 蛋 。 EOS\n",
            "我我我我我我。\n",
            "\n",
            "BOS do you know this song ? EOS\n",
            "BOS 你 知 道 這 首 歌 嗎 ？ EOS\n",
            "我我我我我我的。\n",
            "\n",
            "BOS being rich is n't enough . EOS\n",
            "BOS 有 钱 还 不 够 。 EOS\n",
            "我我我我我我。\n",
            "\n",
            "BOS please tell me about it . EOS\n",
            "BOS 请 告 诉 我 它 的 事 情 。 EOS\n",
            "我我我我我我。\n",
            "\n",
            "BOS he is fluent in french . EOS\n",
            "BOS 他 说 法 语 说 得 很 流 利 。 EOS\n",
            "我我我我我我。\n",
            "\n",
            "BOS that 's what i like . EOS\n",
            "BOS 我 就 喜 欢 它 。 EOS\n",
            "我我我我我我我我。\n",
            "\n",
            "BOS he is an UNK person . EOS\n",
            "BOS 他 是 个 UNK UNK 逼 人 的 的 人 。 EOS\n",
            "我我我我我。\n",
            "\n",
            "BOS my attitude towards him changed . EOS\n",
            "BOS 我 对 他 的 态 度 变 了 。 EOS\n",
            "我我我我我我。\n",
            "\n",
            "BOS she 's not a child . EOS\n",
            "BOS 她 不 是 小 孩 。 EOS\n",
            "我我我我我。\n",
            "\n",
            "BOS she is everything to him . EOS\n",
            "BOS 她 是 他 的 一 切 。 EOS\n",
            "我我我我我我。\n",
            "\n",
            "BOS i heard the door close . EOS\n",
            "BOS 我 听 到 门 关 了 。 EOS\n",
            "我我我我我我我。\n",
            "\n",
            "BOS the topic is worth discussing . EOS\n",
            "BOS 這 是 值 得 探 討 的 話 題 。 EOS\n",
            "我我我我我我。\n",
            "\n",
            "BOS he has a large family . EOS\n",
            "BOS 他 有 一 個 大 家 庭 。 EOS\n",
            "我我我我我我我。\n",
            "\n",
            "BOS my brother became an engineer . EOS\n",
            "BOS 我 哥 哥 成 了 一 名 工 程 師 。 EOS\n",
            "我我我我我我我。\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyx0RraRWmNx",
        "colab_type": "text"
      },
      "source": [
        "## Seq2Seq + attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76e0MlKMWmN0",
        "colab_type": "text"
      },
      "source": [
        "在luong中提到了三种score的计算方法\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5qt-lDm8coOP",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
        "        x_sorted = x[sorted_idx.long()]\n",
        "        embedded = self.dropout(self.embed(x_sorted))\n",
        "        \n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
        "        packed_out, hid = self.rnn(packed_embedded)\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
        "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
        "        out = out[original_idx.long()].contiguous()\n",
        "        hid = hid[:, original_idx.long()].contiguous()\n",
        "        # hid: [2, batch_size, enc_hidden_size]\n",
        "        \n",
        "        hid = torch.cat([hid[-2], hid[-1]], dim=1) # 将最后一层的hid的双向拼接\n",
        "        # hid: [batch_size, 2*enc_hidden_size]\n",
        "        hid = torch.tanh(self.fc(hid)).unsqueeze(0)\n",
        "        # hid: [1, batch_size, dec_hidden_size]\n",
        "        # out: [batch_size, seq_len, 2*enc_hidden_size]\n",
        "        return out, hid"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5PunwHJfcoOT"
      },
      "source": [
        "#### Luong Attention\n",
        "- 根据context vectors和当前的输出hidden states，计算输出"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFUrlxK1WmOA",
        "colab_type": "text"
      },
      "source": [
        "这里我们计算第二种score的计算方法"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "16G1_LB0coOU",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
        "        # enc_hidden_size跟Encoder的一样\n",
        "        super(Attention, self).__init__()\n",
        "        self.enc_hidden_size = enc_hidden_size\n",
        "        self.dec_hidden_size = dec_hidden_size\n",
        "\n",
        "        self.linear_in = nn.Linear(enc_hidden_size*2, dec_hidden_size, bias=False)\n",
        "        self.linear_out = nn.Linear(enc_hidden_size*2 + dec_hidden_size, dec_hidden_size)\n",
        "        \n",
        "    def forward(self, output, context, mask):\n",
        "        # mask = batch_size, output_len, context_len     # mask在Decoder中创建好了\n",
        "        # output: batch_size, output_len, dec_hidden_size，就是Decoder的output\n",
        "        # context: batch_size, context_len, 2*enc_hidden_size，就是Encoder的output\n",
        "        # 这里Encoder网络是双向的，Decoder是单向的\n",
        "    \n",
        "        batch_size = output.size(0)\n",
        "        output_len = output.size(1)\n",
        "        input_len = context.size(1) # input_len = context_len\n",
        "        \n",
        "        # 开始计算score，用到了第二种公式计算方式，先看懂这个网址：https://zhuanlan.zhihu.com/p/40920384\n",
        "        # 通过decoder的hidden states加上encoder的hidden states来计算一个分数，用于计算权重\n",
        "        context_in = self.linear_in(context.view(batch_size*input_len, -1)).view(                \n",
        "            batch_size, input_len, -1) # batch_size, context_len, dec_hidden_size\n",
        "        # 第一步，公式里的Wa先与hs做点乘，把Encoder output的enc_hidden_size换成dec_hidden_size。\n",
        "        \n",
        "        # context_in.transpose(1,2): batch_size, dec_hidden_size, context_len \n",
        "        # output: batch_size, output_len, dec_hidden_size\n",
        "        attn = torch.bmm(output, context_in.transpose(1,2)) \n",
        "        # batch_size, output_len, context_len\n",
        "        # 第二步，ht与上一步结果点乘，得到score\n",
        "\n",
        "        attn.data.masked_fill(mask, -1e6)\n",
        "        # .masked_fill作用请看这个链接：https://blog.csdn.net/candy134834/article/details/84594754\n",
        "        # mask的维度必须和attn维度相同，mask为1的位置对应attn的位置的值替换成-1e6，\n",
        "        # mask为1的意义需要看Decoder函数里面的定义\n",
        "\n",
        "        attn = F.softmax(attn, dim=2) \n",
        "        # batch_size, output_len, context_len\n",
        "        # 这个dim=2到底是怎么softmax的看下下面单元格例子\n",
        "        # 第三步，计算每一个encoder的hidden states对应的权重。\n",
        "        \n",
        "        # context: batch_size, context_len, 2*enc_hidden_size，\n",
        "        context = torch.bmm(attn, context) \n",
        "        # batch_size, output_len, 2*enc_hidden_size\n",
        "        # 第四步，得出context vector是一个对于encoder输出的hidden states的一个加权平均\n",
        "        \n",
        "        # output: batch_size, output_len, dec_hidden_size\n",
        "        output = torch.cat((context, output), dim=2) \n",
        "        # output：batch_size, output_len, 2*enc_hidden_size+dec_hidden_size\n",
        "        # 第五步，将context vector和 decoder的hidden states 串起来。\n",
        "        \n",
        "        output = output.view(batch_size*output_len, -1)\n",
        "        # output.shape = (batch_size*output_len, 2*enc_hidden_size+dec_hidden_size)\n",
        "        output = torch.tanh(self.linear_out(output)) \n",
        "        # output.shape=(batch_size*output_len, dec_hidden_size)\n",
        "        output = output.view(batch_size, output_len, -1)\n",
        "        # output.shape=(batch_size, output_len, dec_hidden_size)\n",
        "        # attn.shape = batch_size, output_len, context_len\n",
        "        return output, attn"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LzcOPJf_coOX"
      },
      "source": [
        "#### Decoder\n",
        "- decoder会根据已经翻译的句子内容，和context vectors，来决定下一个输出的单词"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gX8flwywcoOY",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
        "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def create_mask(self, x_len, y_len):\n",
        "        # x_len 是一个batch中文句子的长度列表\n",
        "        # y_len 是一个batch英文句子的长度列表\n",
        "        # a mask of shape x_len * y_len\n",
        "        device = x_len.device\n",
        "        max_x_len = x_len.max()\n",
        "        max_y_len = y_len.max()\n",
        "        \n",
        "        x_mask = torch.arange(max_x_len, device=device)[None, :] < x_len[:, None]\n",
        "        # print(x_mask.shape) = (batch_size, output_len) # 中文句子的mask\n",
        "        y_mask = torch.arange(max_y_len, device=device)[None, :] < y_len[:, None]\n",
        "        # print(y_mask.shape) = (batch_size, context_len) # 英文句子的mask\n",
        "        \n",
        "        mask = ( ~ x_mask[:, :, None] * y_mask[:, None, :]).byte()\n",
        "        # mask = (1 - x_mask[:, :, None] * y_mask[:, None, :]).byte()\n",
        "        # 1-说明取反\n",
        "        # x_mask[:, :, None] = (batch_size, output_len, 1)\n",
        "        # y_mask[:, None, :] =  (batch_size, 1, context_len)\n",
        "        # print(mask.shape) = (batch_size, output_len, context_len)\n",
        "        # 注意这个例子的*相乘不是torch.bmm矩阵点乘，只是用到了广播机制而已。\n",
        "        return mask\n",
        "    \n",
        "    def forward(self, encoder_out, x_lengths, y, y_lengths, hid):\n",
        "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
        "        y_sorted = y[sorted_idx.long()]\n",
        "        hid = hid[:, sorted_idx.long()]\n",
        "        \n",
        "        y_sorted = self.dropout(self.embed(y_sorted)) # batch_size, output_length, embed_size\n",
        "\n",
        "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
        "        out, hid = self.rnn(packed_seq, hid)\n",
        "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
        "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
        "        output_seq = unpacked[original_idx.long()].contiguous()\n",
        "        hid = hid[:, original_idx.long()].contiguous()\n",
        "\n",
        "        mask = self.create_mask(y_lengths, x_lengths) # 这里真是坑，第一个参数位置是中文句子的长度列表\n",
        "\n",
        "        output, attn = self.attention(output_seq, encoder_out, mask) \n",
        "        # output.shape=(batch_size, output_len, dec_hidden_size)\n",
        "        # attn.shape = batch_size, output_len, context_len\n",
        "        \n",
        "        # self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
        "        output = F.log_softmax(self.out(output), -1) # 计算最后的输出概率\n",
        "        # output =(batch_size, output_len, vocab_size)\n",
        "        # 最后一个vocab_size维度 log_softmax\n",
        "        # hid.shape = (1, batch_size, dec_hidden_size)\n",
        "        return output, hid, attn"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1kfN48PjcoOe"
      },
      "source": [
        "#### Seq2Seq\n",
        "- 最后我们构建Seq2Seq模型把encoder, attention, decoder串到一起"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xmsUCoGGcoOf",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "    def forward(self, x, x_lengths, y, y_lengths):\n",
        "        encoder_out, hid = self.encoder(x, x_lengths)\n",
        "        # print(hid.shape)=torch.Size([1, batch_size, dec_hidden_size])\n",
        "        # print(out.shape)=torch.Size([batch_size, seq_len, 2*enc_hidden_size])\n",
        "        output, hid, attn = self.decoder(encoder_out=encoder_out, \n",
        "                    x_lengths=x_lengths,\n",
        "                    y=y,\n",
        "                    y_lengths=y_lengths,\n",
        "                    hid=hid)\n",
        "        # output =(batch_size, output_len, vocab_size)\n",
        "        # hid.shape = (1, batch_size, dec_hidden_size)\n",
        "        # attn.shape = (batch_size, output_len, context_len)\n",
        "        return output, attn\n",
        "    \n",
        "\n",
        "    def translate(self, x, x_lengths, y, max_length=100):\n",
        "        encoder_out, hid = self.encoder(x, x_lengths)\n",
        "        preds = []\n",
        "        batch_size = x.shape[0]\n",
        "        attns = []\n",
        "        for i in range(max_length):\n",
        "            output, hid, attn = self.decoder(encoder_out=encoder_out, \n",
        "                    x_lengths=x_lengths,\n",
        "                    y=y,\n",
        "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
        "                    hid=hid)\n",
        "            y = output.max(2)[1].view(batch_size, 1)\n",
        "            preds.append(y)\n",
        "            attns.append(attn)\n",
        "        return torch.cat(preds, 1), torch.cat(attns, 1)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GhMzsG85coOk"
      },
      "source": [
        "训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jar62Gi3coOl",
        "colab": {}
      },
      "source": [
        "dropout = 0.2\n",
        "embed_size = hidden_size = 100\n",
        "encoder = Encoder(vocab_size=en_total_words,\n",
        "                    embed_size=embed_size,\n",
        "                    enc_hidden_size=hidden_size,\n",
        "                    dec_hidden_size=hidden_size,\n",
        "                    dropout=dropout)\n",
        "decoder = Decoder(vocab_size=cn_total_words,\n",
        "                    embed_size=embed_size,\n",
        "                    enc_hidden_size=hidden_size,\n",
        "                    dec_hidden_size=hidden_size,\n",
        "                    dropout=dropout)\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "model = model.to(device)\n",
        "loss_fn = LanguageModelCriterion().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HOxvWAiscoOo",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f6a4330e-250d-4a3c-bb00-ebbaf098d132"
      },
      "source": [
        "train(model, train_data, num_epochs=100)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 iteration 0 loss 5.070350646972656\n",
            "Epoch 0 iteration 100 loss 4.820558547973633\n",
            "Epoch 0 iteration 200 loss 5.059200286865234\n",
            "Epoch 0 Training loss 4.988149736244423\n",
            "Evaluation loss 4.678154568269887\n",
            "Epoch 1 iteration 0 loss 4.378211975097656\n",
            "Epoch 1 iteration 100 loss 4.305964469909668\n",
            "Epoch 1 iteration 200 loss 4.6104865074157715\n",
            "Epoch 1 Training loss 4.472330907373058\n",
            "Epoch 2 iteration 0 loss 3.9458651542663574\n",
            "Epoch 2 iteration 100 loss 3.997627019882202\n",
            "Epoch 2 iteration 200 loss 4.32684326171875\n",
            "Epoch 2 Training loss 4.110083308674354\n",
            "Epoch 3 iteration 0 loss 3.6405317783355713\n",
            "Epoch 3 iteration 100 loss 3.7315287590026855\n",
            "Epoch 3 iteration 200 loss 4.062915325164795\n",
            "Epoch 3 Training loss 3.8465015012228463\n",
            "Epoch 4 iteration 0 loss 3.433096408843994\n",
            "Epoch 4 iteration 100 loss 3.513590097427368\n",
            "Epoch 4 iteration 200 loss 3.9205565452575684\n",
            "Epoch 4 Training loss 3.6414518059011396\n",
            "Epoch 5 iteration 0 loss 3.231971263885498\n",
            "Epoch 5 iteration 100 loss 3.33308482170105\n",
            "Epoch 5 iteration 200 loss 3.747715473175049\n",
            "Epoch 5 Training loss 3.4685063184426386\n",
            "Evaluation loss 3.5524308152155717\n",
            "Epoch 6 iteration 0 loss 3.045614242553711\n",
            "Epoch 6 iteration 100 loss 3.2115159034729004\n",
            "Epoch 6 iteration 200 loss 3.6362829208374023\n",
            "Epoch 6 Training loss 3.321865841723491\n",
            "Epoch 7 iteration 0 loss 2.918537139892578\n",
            "Epoch 7 iteration 100 loss 3.0683867931365967\n",
            "Epoch 7 iteration 200 loss 3.5033748149871826\n",
            "Epoch 7 Training loss 3.194381697159496\n",
            "Epoch 8 iteration 0 loss 2.8030903339385986\n",
            "Epoch 8 iteration 100 loss 2.930556058883667\n",
            "Epoch 8 iteration 200 loss 3.414945363998413\n",
            "Epoch 8 Training loss 3.0818151163573826\n",
            "Epoch 9 iteration 0 loss 2.689337968826294\n",
            "Epoch 9 iteration 100 loss 2.8165934085845947\n",
            "Epoch 9 iteration 200 loss 3.3430402278900146\n",
            "Epoch 9 Training loss 2.9803752474258998\n",
            "Epoch 10 iteration 0 loss 2.6162142753601074\n",
            "Epoch 10 iteration 100 loss 2.7009434700012207\n",
            "Epoch 10 iteration 200 loss 3.196560859680176\n",
            "Epoch 10 Training loss 2.8908802495363317\n",
            "Evaluation loss 3.247826691639525\n",
            "Epoch 11 iteration 0 loss 2.514803409576416\n",
            "Epoch 11 iteration 100 loss 2.64622163772583\n",
            "Epoch 11 iteration 200 loss 3.2106809616088867\n",
            "Epoch 11 Training loss 2.807239356172807\n",
            "Epoch 12 iteration 0 loss 2.4626035690307617\n",
            "Epoch 12 iteration 100 loss 2.5527870655059814\n",
            "Epoch 12 iteration 200 loss 3.143561363220215\n",
            "Epoch 12 Training loss 2.732851507186788\n",
            "Epoch 13 iteration 0 loss 2.382215976715088\n",
            "Epoch 13 iteration 100 loss 2.4752089977264404\n",
            "Epoch 13 iteration 200 loss 2.9899725914001465\n",
            "Epoch 13 Training loss 2.6621246798647604\n",
            "Epoch 14 iteration 0 loss 2.2933568954467773\n",
            "Epoch 14 iteration 100 loss 2.3937389850616455\n",
            "Epoch 14 iteration 200 loss 2.9585061073303223\n",
            "Epoch 14 Training loss 2.5984274894639343\n",
            "Epoch 15 iteration 0 loss 2.2403295040130615\n",
            "Epoch 15 iteration 100 loss 2.2793688774108887\n",
            "Epoch 15 iteration 200 loss 2.876659393310547\n",
            "Epoch 15 Training loss 2.5373089519305525\n",
            "Evaluation loss 3.1005258886054676\n",
            "Epoch 16 iteration 0 loss 2.2015554904937744\n",
            "Epoch 16 iteration 100 loss 2.260180950164795\n",
            "Epoch 16 iteration 200 loss 2.8232436180114746\n",
            "Epoch 16 Training loss 2.4812657786747354\n",
            "Epoch 17 iteration 0 loss 2.1269116401672363\n",
            "Epoch 17 iteration 100 loss 2.180694103240967\n",
            "Epoch 17 iteration 200 loss 2.7837016582489014\n",
            "Epoch 17 Training loss 2.4275206623185164\n",
            "Epoch 18 iteration 0 loss 2.0371341705322266\n",
            "Epoch 18 iteration 100 loss 2.11972713470459\n",
            "Epoch 18 iteration 200 loss 2.7231030464172363\n",
            "Epoch 18 Training loss 2.374840282805137\n",
            "Epoch 19 iteration 0 loss 2.038038730621338\n",
            "Epoch 19 iteration 100 loss 2.0749213695526123\n",
            "Epoch 19 iteration 200 loss 2.686671495437622\n",
            "Epoch 19 Training loss 2.328320879261261\n",
            "Epoch 20 iteration 0 loss 1.9933955669403076\n",
            "Epoch 20 iteration 100 loss 2.015784502029419\n",
            "Epoch 20 iteration 200 loss 2.6580355167388916\n",
            "Epoch 20 Training loss 2.283638168561129\n",
            "Evaluation loss 3.0344090417315717\n",
            "Epoch 21 iteration 0 loss 1.9361672401428223\n",
            "Epoch 21 iteration 100 loss 1.9529626369476318\n",
            "Epoch 21 iteration 200 loss 2.604079008102417\n",
            "Epoch 21 Training loss 2.2403320812650946\n",
            "Epoch 22 iteration 0 loss 1.8959698677062988\n",
            "Epoch 22 iteration 100 loss 1.8620413541793823\n",
            "Epoch 22 iteration 200 loss 2.549434185028076\n",
            "Epoch 22 Training loss 2.2000973823681624\n",
            "Epoch 23 iteration 0 loss 1.904073715209961\n",
            "Epoch 23 iteration 100 loss 1.8901786804199219\n",
            "Epoch 23 iteration 200 loss 2.5189285278320312\n",
            "Epoch 23 Training loss 2.1604536385173048\n",
            "Epoch 24 iteration 0 loss 1.8221869468688965\n",
            "Epoch 24 iteration 100 loss 1.7490034103393555\n",
            "Epoch 24 iteration 200 loss 2.5051462650299072\n",
            "Epoch 24 Training loss 2.12271532137333\n",
            "Epoch 25 iteration 0 loss 1.792157769203186\n",
            "Epoch 25 iteration 100 loss 1.745448350906372\n",
            "Epoch 25 iteration 200 loss 2.416353940963745\n",
            "Epoch 25 Training loss 2.085510235126571\n",
            "Evaluation loss 3.012935219890143\n",
            "Epoch 26 iteration 0 loss 1.7301499843597412\n",
            "Epoch 26 iteration 100 loss 1.7323261499404907\n",
            "Epoch 26 iteration 200 loss 2.445427417755127\n",
            "Epoch 26 Training loss 2.0516151113154772\n",
            "Epoch 27 iteration 0 loss 1.6943188905715942\n",
            "Epoch 27 iteration 100 loss 1.6619443893432617\n",
            "Epoch 27 iteration 200 loss 2.3312392234802246\n",
            "Epoch 27 Training loss 2.018444679853272\n",
            "Epoch 28 iteration 0 loss 1.6469300985336304\n",
            "Epoch 28 iteration 100 loss 1.6616876125335693\n",
            "Epoch 28 iteration 200 loss 2.3489811420440674\n",
            "Epoch 28 Training loss 1.987167905013721\n",
            "Epoch 29 iteration 0 loss 1.6112520694732666\n",
            "Epoch 29 iteration 100 loss 1.6192384958267212\n",
            "Epoch 29 iteration 200 loss 2.292145252227783\n",
            "Epoch 29 Training loss 1.955642337988404\n",
            "Epoch 30 iteration 0 loss 1.5568615198135376\n",
            "Epoch 30 iteration 100 loss 1.6042102575302124\n",
            "Epoch 30 iteration 200 loss 2.2640254497528076\n",
            "Epoch 30 Training loss 1.9227967677706406\n",
            "Evaluation loss 3.0057116538029147\n",
            "Epoch 31 iteration 0 loss 1.576025366783142\n",
            "Epoch 31 iteration 100 loss 1.5380747318267822\n",
            "Epoch 31 iteration 200 loss 2.2912447452545166\n",
            "Epoch 31 Training loss 1.9053877110770936\n",
            "Epoch 32 iteration 0 loss 1.5507079362869263\n",
            "Epoch 32 iteration 100 loss 1.5570087432861328\n",
            "Epoch 32 iteration 200 loss 2.1763551235198975\n",
            "Epoch 32 Training loss 1.8717647138760705\n",
            "Epoch 33 iteration 0 loss 1.4776815176010132\n",
            "Epoch 33 iteration 100 loss 1.5070013999938965\n",
            "Epoch 33 iteration 200 loss 2.1715080738067627\n",
            "Epoch 33 Training loss 1.8478175987733567\n",
            "Epoch 34 iteration 0 loss 1.4630179405212402\n",
            "Epoch 34 iteration 100 loss 1.5415295362472534\n",
            "Epoch 34 iteration 200 loss 2.1843314170837402\n",
            "Epoch 34 Training loss 1.8229996889171423\n",
            "Epoch 35 iteration 0 loss 1.4203025102615356\n",
            "Epoch 35 iteration 100 loss 1.4518847465515137\n",
            "Epoch 35 iteration 200 loss 2.1352040767669678\n",
            "Epoch 35 Training loss 1.7940268518611018\n",
            "Evaluation loss 3.0089234384869576\n",
            "Epoch 36 iteration 0 loss 1.4475277662277222\n",
            "Epoch 36 iteration 100 loss 1.4472365379333496\n",
            "Epoch 36 iteration 200 loss 2.1173040866851807\n",
            "Epoch 36 Training loss 1.7715329811995004\n",
            "Epoch 37 iteration 0 loss 1.3795528411865234\n",
            "Epoch 37 iteration 100 loss 1.4394657611846924\n",
            "Epoch 37 iteration 200 loss 2.1031081676483154\n",
            "Epoch 37 Training loss 1.7485406562300698\n",
            "Epoch 38 iteration 0 loss 1.3518015146255493\n",
            "Epoch 38 iteration 100 loss 1.3795233964920044\n",
            "Epoch 38 iteration 200 loss 2.061619758605957\n",
            "Epoch 38 Training loss 1.7248465308192305\n",
            "Epoch 39 iteration 0 loss 1.378030776977539\n",
            "Epoch 39 iteration 100 loss 1.3478801250457764\n",
            "Epoch 39 iteration 200 loss 2.0523993968963623\n",
            "Epoch 39 Training loss 1.7077698364897664\n",
            "Epoch 40 iteration 0 loss 1.335542917251587\n",
            "Epoch 40 iteration 100 loss 1.343032717704773\n",
            "Epoch 40 iteration 200 loss 2.047027349472046\n",
            "Epoch 40 Training loss 1.6828222662830488\n",
            "Evaluation loss 3.019167257281313\n",
            "Epoch 41 iteration 0 loss 1.2722305059432983\n",
            "Epoch 41 iteration 100 loss 1.3250147104263306\n",
            "Epoch 41 iteration 200 loss 1.965550422668457\n",
            "Epoch 41 Training loss 1.6657925071383144\n",
            "Epoch 42 iteration 0 loss 1.3206872940063477\n",
            "Epoch 42 iteration 100 loss 1.299448847770691\n",
            "Epoch 42 iteration 200 loss 1.9710954427719116\n",
            "Epoch 42 Training loss 1.6455860371987165\n",
            "Epoch 43 iteration 0 loss 1.285097599029541\n",
            "Epoch 43 iteration 100 loss 1.2931063175201416\n",
            "Epoch 43 iteration 200 loss 1.9357976913452148\n",
            "Epoch 43 Training loss 1.6242825094206184\n",
            "Epoch 44 iteration 0 loss 1.2285470962524414\n",
            "Epoch 44 iteration 100 loss 1.2616078853607178\n",
            "Epoch 44 iteration 200 loss 1.8613090515136719\n",
            "Epoch 44 Training loss 1.605806061006056\n",
            "Epoch 45 iteration 0 loss 1.2021993398666382\n",
            "Epoch 45 iteration 100 loss 1.300480842590332\n",
            "Epoch 45 iteration 200 loss 1.8581916093826294\n",
            "Epoch 45 Training loss 1.5843672275658531\n",
            "Evaluation loss 3.032219372706836\n",
            "Epoch 46 iteration 0 loss 1.2204270362854004\n",
            "Epoch 46 iteration 100 loss 1.1800646781921387\n",
            "Epoch 46 iteration 200 loss 1.9093990325927734\n",
            "Epoch 46 Training loss 1.5710013438418762\n",
            "Epoch 47 iteration 0 loss 1.171179175376892\n",
            "Epoch 47 iteration 100 loss 1.1896157264709473\n",
            "Epoch 47 iteration 200 loss 1.8660122156143188\n",
            "Epoch 47 Training loss 1.554466197131872\n",
            "Epoch 48 iteration 0 loss 1.190609097480774\n",
            "Epoch 48 iteration 100 loss 1.1699604988098145\n",
            "Epoch 48 iteration 200 loss 1.8990025520324707\n",
            "Epoch 48 Training loss 1.536812117527099\n",
            "Epoch 49 iteration 0 loss 1.1347819566726685\n",
            "Epoch 49 iteration 100 loss 1.1992888450622559\n",
            "Epoch 49 iteration 200 loss 1.8291341066360474\n",
            "Epoch 49 Training loss 1.5195645638883477\n",
            "Epoch 50 iteration 0 loss 1.1621776819229126\n",
            "Epoch 50 iteration 100 loss 1.1366691589355469\n",
            "Epoch 50 iteration 200 loss 1.7839405536651611\n",
            "Epoch 50 Training loss 1.5002408335039898\n",
            "Evaluation loss 3.0490408809640255\n",
            "Epoch 51 iteration 0 loss 1.1004716157913208\n",
            "Epoch 51 iteration 100 loss 1.0848424434661865\n",
            "Epoch 51 iteration 200 loss 1.7821482419967651\n",
            "Epoch 51 Training loss 1.486894716466308\n",
            "Epoch 52 iteration 0 loss 1.113624930381775\n",
            "Epoch 52 iteration 100 loss 1.1172170639038086\n",
            "Epoch 52 iteration 200 loss 1.783369541168213\n",
            "Epoch 52 Training loss 1.471296141556335\n",
            "Epoch 53 iteration 0 loss 1.0583155155181885\n",
            "Epoch 53 iteration 100 loss 1.126234531402588\n",
            "Epoch 53 iteration 200 loss 1.7678284645080566\n",
            "Epoch 53 Training loss 1.4578569138384652\n",
            "Epoch 54 iteration 0 loss 1.1202415227890015\n",
            "Epoch 54 iteration 100 loss 1.0739927291870117\n",
            "Epoch 54 iteration 200 loss 1.743609070777893\n",
            "Epoch 54 Training loss 1.447415913935627\n",
            "Epoch 55 iteration 0 loss 1.005126714706421\n",
            "Epoch 55 iteration 100 loss 1.0664958953857422\n",
            "Epoch 55 iteration 200 loss 1.7263078689575195\n",
            "Epoch 55 Training loss 1.4298366662217354\n",
            "Evaluation loss 3.0780647793462554\n",
            "Epoch 56 iteration 0 loss 1.0328119993209839\n",
            "Epoch 56 iteration 100 loss 1.0269381999969482\n",
            "Epoch 56 iteration 200 loss 1.711896538734436\n",
            "Epoch 56 Training loss 1.4155632438754828\n",
            "Epoch 57 iteration 0 loss 0.9860355854034424\n",
            "Epoch 57 iteration 100 loss 1.034627914428711\n",
            "Epoch 57 iteration 200 loss 1.7367409467697144\n",
            "Epoch 57 Training loss 1.4037403800908566\n",
            "Epoch 58 iteration 0 loss 1.015838623046875\n",
            "Epoch 58 iteration 100 loss 1.010921597480774\n",
            "Epoch 58 iteration 200 loss 1.6859478950500488\n",
            "Epoch 58 Training loss 1.3894971327008565\n",
            "Epoch 59 iteration 0 loss 1.0102308988571167\n",
            "Epoch 59 iteration 100 loss 1.004539966583252\n",
            "Epoch 59 iteration 200 loss 1.6645151376724243\n",
            "Epoch 59 Training loss 1.3757502728906414\n",
            "Epoch 60 iteration 0 loss 0.9040292501449585\n",
            "Epoch 60 iteration 100 loss 1.0408172607421875\n",
            "Epoch 60 iteration 200 loss 1.689808964729309\n",
            "Epoch 60 Training loss 1.3604046518706674\n",
            "Evaluation loss 3.1070478430374995\n",
            "Epoch 61 iteration 0 loss 0.9631772637367249\n",
            "Epoch 61 iteration 100 loss 0.9880275130271912\n",
            "Epoch 61 iteration 200 loss 1.6689567565917969\n",
            "Epoch 61 Training loss 1.3493494028903459\n",
            "Epoch 62 iteration 0 loss 0.9835140109062195\n",
            "Epoch 62 iteration 100 loss 0.9981334209442139\n",
            "Epoch 62 iteration 200 loss 1.6732382774353027\n",
            "Epoch 62 Training loss 1.3375289361712297\n",
            "Epoch 63 iteration 0 loss 0.9177541136741638\n",
            "Epoch 63 iteration 100 loss 0.991683840751648\n",
            "Epoch 63 iteration 200 loss 1.6529173851013184\n",
            "Epoch 63 Training loss 1.3245781397344916\n",
            "Epoch 64 iteration 0 loss 0.9430961012840271\n",
            "Epoch 64 iteration 100 loss 1.0056464672088623\n",
            "Epoch 64 iteration 200 loss 1.5752886533737183\n",
            "Epoch 64 Training loss 1.3111070255147321\n",
            "Epoch 65 iteration 0 loss 0.9193617701530457\n",
            "Epoch 65 iteration 100 loss 0.9731063842773438\n",
            "Epoch 65 iteration 200 loss 1.5860095024108887\n",
            "Epoch 65 Training loss 1.3005898353301621\n",
            "Evaluation loss 3.1344845612673473\n",
            "Epoch 66 iteration 0 loss 0.8612948656082153\n",
            "Epoch 66 iteration 100 loss 0.9298420548439026\n",
            "Epoch 66 iteration 200 loss 1.5870704650878906\n",
            "Epoch 66 Training loss 1.2928555090551432\n",
            "Epoch 67 iteration 0 loss 0.9007643461227417\n",
            "Epoch 67 iteration 100 loss 0.9502060413360596\n",
            "Epoch 67 iteration 200 loss 1.5976848602294922\n",
            "Epoch 67 Training loss 1.2810681759341405\n",
            "Epoch 68 iteration 0 loss 0.904141366481781\n",
            "Epoch 68 iteration 100 loss 0.9651005268096924\n",
            "Epoch 68 iteration 200 loss 1.5104542970657349\n",
            "Epoch 68 Training loss 1.272242462980771\n",
            "Epoch 69 iteration 0 loss 0.8625770807266235\n",
            "Epoch 69 iteration 100 loss 0.9052244424819946\n",
            "Epoch 69 iteration 200 loss 1.5296518802642822\n",
            "Epoch 69 Training loss 1.2620076196029373\n",
            "Epoch 70 iteration 0 loss 0.8743317723274231\n",
            "Epoch 70 iteration 100 loss 0.9359579086303711\n",
            "Epoch 70 iteration 200 loss 1.5253866910934448\n",
            "Epoch 70 Training loss 1.2503886101051127\n",
            "Evaluation loss 3.16357128620026\n",
            "Epoch 71 iteration 0 loss 0.8658103346824646\n",
            "Epoch 71 iteration 100 loss 0.9165797829627991\n",
            "Epoch 71 iteration 200 loss 1.5562275648117065\n",
            "Epoch 71 Training loss 1.2426427011712389\n",
            "Epoch 72 iteration 0 loss 0.8727307915687561\n",
            "Epoch 72 iteration 100 loss 0.8838544487953186\n",
            "Epoch 72 iteration 200 loss 1.5069340467453003\n",
            "Epoch 72 Training loss 1.2303605728944107\n",
            "Epoch 73 iteration 0 loss 0.8904566168785095\n",
            "Epoch 73 iteration 100 loss 0.8673937916755676\n",
            "Epoch 73 iteration 200 loss 1.5405542850494385\n",
            "Epoch 73 Training loss 1.2163196343241252\n",
            "Epoch 74 iteration 0 loss 0.8326627612113953\n",
            "Epoch 74 iteration 100 loss 0.8431721329689026\n",
            "Epoch 74 iteration 200 loss 1.5550968647003174\n",
            "Epoch 74 Training loss 1.2080989721361715\n",
            "Epoch 75 iteration 0 loss 0.8495824933052063\n",
            "Epoch 75 iteration 100 loss 0.8251366019248962\n",
            "Epoch 75 iteration 200 loss 1.4777411222457886\n",
            "Epoch 75 Training loss 1.1986094831391845\n",
            "Evaluation loss 3.181987347272103\n",
            "Epoch 76 iteration 0 loss 0.7865337133407593\n",
            "Epoch 76 iteration 100 loss 0.9266216158866882\n",
            "Epoch 76 iteration 200 loss 1.4591412544250488\n",
            "Epoch 76 Training loss 1.1932196639021988\n",
            "Epoch 77 iteration 0 loss 0.7945457696914673\n",
            "Epoch 77 iteration 100 loss 0.837568461894989\n",
            "Epoch 77 iteration 200 loss 1.4992955923080444\n",
            "Epoch 77 Training loss 1.1822143839924233\n",
            "Epoch 78 iteration 0 loss 0.8477261066436768\n",
            "Epoch 78 iteration 100 loss 0.8449146151542664\n",
            "Epoch 78 iteration 200 loss 1.3925931453704834\n",
            "Epoch 78 Training loss 1.1712349566420108\n",
            "Epoch 79 iteration 0 loss 0.8118950128555298\n",
            "Epoch 79 iteration 100 loss 0.7954407334327698\n",
            "Epoch 79 iteration 200 loss 1.444852352142334\n",
            "Epoch 79 Training loss 1.1670867625299421\n",
            "Epoch 80 iteration 0 loss 0.7648046016693115\n",
            "Epoch 80 iteration 100 loss 0.784464955329895\n",
            "Epoch 80 iteration 200 loss 1.4420181512832642\n",
            "Epoch 80 Training loss 1.1537796690816324\n",
            "Evaluation loss 3.2139114902060024\n",
            "Epoch 81 iteration 0 loss 0.754738986492157\n",
            "Epoch 81 iteration 100 loss 0.8263189196586609\n",
            "Epoch 81 iteration 200 loss 1.4142329692840576\n",
            "Epoch 81 Training loss 1.144089519618697\n",
            "Epoch 82 iteration 0 loss 0.8229148983955383\n",
            "Epoch 82 iteration 100 loss 0.8271111845970154\n",
            "Epoch 82 iteration 200 loss 1.416876196861267\n",
            "Epoch 82 Training loss 1.1379717249384795\n",
            "Epoch 83 iteration 0 loss 0.7631769776344299\n",
            "Epoch 83 iteration 100 loss 0.8094179630279541\n",
            "Epoch 83 iteration 200 loss 1.4439959526062012\n",
            "Epoch 83 Training loss 1.1331734906257438\n",
            "Epoch 84 iteration 0 loss 0.8061301708221436\n",
            "Epoch 84 iteration 100 loss 0.8358878493309021\n",
            "Epoch 84 iteration 200 loss 1.4300754070281982\n",
            "Epoch 84 Training loss 1.1210030866725762\n",
            "Epoch 85 iteration 0 loss 0.7451398372650146\n",
            "Epoch 85 iteration 100 loss 0.7244279384613037\n",
            "Epoch 85 iteration 200 loss 1.3744946718215942\n",
            "Epoch 85 Training loss 1.11147024059461\n",
            "Evaluation loss 3.2327538797527193\n",
            "Epoch 86 iteration 0 loss 0.7152448296546936\n",
            "Epoch 86 iteration 100 loss 0.7980808615684509\n",
            "Epoch 86 iteration 200 loss 1.4080595970153809\n",
            "Epoch 86 Training loss 1.1047646410392784\n",
            "Epoch 87 iteration 0 loss 0.7213557958602905\n",
            "Epoch 87 iteration 100 loss 0.7920738458633423\n",
            "Epoch 87 iteration 200 loss 1.4455478191375732\n",
            "Epoch 87 Training loss 1.0954635203464635\n",
            "Epoch 88 iteration 0 loss 0.7431939840316772\n",
            "Epoch 88 iteration 100 loss 0.7023082375526428\n",
            "Epoch 88 iteration 200 loss 1.3889566659927368\n",
            "Epoch 88 Training loss 1.0927493063029419\n",
            "Epoch 89 iteration 0 loss 0.7450563311576843\n",
            "Epoch 89 iteration 100 loss 0.8208162188529968\n",
            "Epoch 89 iteration 200 loss 1.3397287130355835\n",
            "Epoch 89 Training loss 1.0778995149734092\n",
            "Epoch 90 iteration 0 loss 0.7828144431114197\n",
            "Epoch 90 iteration 100 loss 0.7431156635284424\n",
            "Epoch 90 iteration 200 loss 1.3472706079483032\n",
            "Epoch 90 Training loss 1.0771844412662246\n",
            "Evaluation loss 3.2684969379277273\n",
            "Epoch 91 iteration 0 loss 0.7327263355255127\n",
            "Epoch 91 iteration 100 loss 0.6990042328834534\n",
            "Epoch 91 iteration 200 loss 1.336659550666809\n",
            "Epoch 91 Training loss 1.0704722197522736\n",
            "Epoch 92 iteration 0 loss 0.76531982421875\n",
            "Epoch 92 iteration 100 loss 0.7582399845123291\n",
            "Epoch 92 iteration 200 loss 1.3540548086166382\n",
            "Epoch 92 Training loss 1.0614797662581452\n",
            "Epoch 93 iteration 0 loss 0.7272782921791077\n",
            "Epoch 93 iteration 100 loss 0.7658087611198425\n",
            "Epoch 93 iteration 200 loss 1.3784801959991455\n",
            "Epoch 93 Training loss 1.055269220298442\n",
            "Epoch 94 iteration 0 loss 0.6914409399032593\n",
            "Epoch 94 iteration 100 loss 0.7104113698005676\n",
            "Epoch 94 iteration 200 loss 1.3256922960281372\n",
            "Epoch 94 Training loss 1.046365323342712\n",
            "Epoch 95 iteration 0 loss 0.7152050733566284\n",
            "Epoch 95 iteration 100 loss 0.698679506778717\n",
            "Epoch 95 iteration 200 loss 1.3199938535690308\n",
            "Epoch 95 Training loss 1.0380035288767266\n",
            "Evaluation loss 3.2824566676898614\n",
            "Epoch 96 iteration 0 loss 0.6955462694168091\n",
            "Epoch 96 iteration 100 loss 0.7670145630836487\n",
            "Epoch 96 iteration 200 loss 1.2907215356826782\n",
            "Epoch 96 Training loss 1.033234768403742\n",
            "Epoch 97 iteration 0 loss 0.6821770071983337\n",
            "Epoch 97 iteration 100 loss 0.6774928569793701\n",
            "Epoch 97 iteration 200 loss 1.3037949800491333\n",
            "Epoch 97 Training loss 1.0263628207884523\n",
            "Epoch 98 iteration 0 loss 0.7081736922264099\n",
            "Epoch 98 iteration 100 loss 0.6551257967948914\n",
            "Epoch 98 iteration 200 loss 1.2879198789596558\n",
            "Epoch 98 Training loss 1.018811948948441\n",
            "Epoch 99 iteration 0 loss 0.684575617313385\n",
            "Epoch 99 iteration 100 loss 0.6964427828788757\n",
            "Epoch 99 iteration 200 loss 1.2791823148727417\n",
            "Epoch 99 Training loss 1.0091356497801125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZEhDVMR9coOr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "165581a4-7784-42d3-a25b-dee008696b3e"
      },
      "source": [
        "for i in range(100,120):\n",
        "    translate_dev(i)\n",
        "    print()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOS you have nice skin . EOS\n",
            "BOS 你 的 皮 膚 真 好 。 EOS\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "你最好有很多新鲜事。\n",
            "\n",
            "BOS you 're UNK correct . EOS\n",
            "BOS 你 部 分 正 确 。 EOS\n",
            "你的生身。\n",
            "\n",
            "BOS everyone admired his courage . EOS\n",
            "BOS 每 個 人 都 佩 服 他 的 勇 氣 。 EOS\n",
            "每個人都認釋了他的意見。\n",
            "\n",
            "BOS what time is it ? EOS\n",
            "BOS 几 点 了 ？ EOS\n",
            "多少钱？\n",
            "\n",
            "BOS i 'm free tonight . EOS\n",
            "BOS 我 今 晚 有 空 。 EOS\n",
            "我今晚有空。\n",
            "\n",
            "BOS here is your book . EOS\n",
            "BOS 這 是 你 的 書 。 EOS\n",
            "你的書在這裡。\n",
            "\n",
            "BOS they are at lunch . EOS\n",
            "BOS 他 们 在 吃 午 饭 。 EOS\n",
            "他们午吃午饭。\n",
            "\n",
            "BOS this chair is UNK . EOS\n",
            "BOS 這 把 椅 子 很 UNK 。 EOS\n",
            "这里的发生是门。\n",
            "\n",
            "BOS it 's pretty heavy . EOS\n",
            "BOS 它 真 重 。 EOS\n",
            "它是居机场的。\n",
            "\n",
            "BOS many attended his funeral . EOS\n",
            "BOS 很 多 人 都 参 加 了 他 的 葬 礼 。 EOS\n",
            "每个人都知道他的音樂。\n",
            "\n",
            "BOS training will be provided . EOS\n",
            "BOS 会 有 训 练 。 EOS\n",
            "即待有空光。\n",
            "\n",
            "BOS someone is watching you . EOS\n",
            "BOS 有 人 在 看 著 你 。 EOS\n",
            "有人在看你。\n",
            "\n",
            "BOS i slapped his face . EOS\n",
            "BOS 我 摑 了 他 的 臉 。 EOS\n",
            "我愛他打斷了。\n",
            "\n",
            "BOS i like UNK music . EOS\n",
            "BOS 我 喜 歡 流 行 音 樂 。 EOS\n",
            "我喜欢阅读。\n",
            "\n",
            "BOS tom had no children . EOS\n",
            "BOS T o m 沒 有 孩 子 。 EOS\n",
            "汤姆没有孩子。\n",
            "\n",
            "BOS please lock the door . EOS\n",
            "BOS 請 把 門 鎖 上 。 EOS\n",
            "請關門門。\n",
            "\n",
            "BOS tom has calmed down . EOS\n",
            "BOS 汤 姆 冷 静 下 来 了 。 EOS\n",
            "Tom有三個走。\n",
            "\n",
            "BOS please speak more loudly . EOS\n",
            "BOS 請 說 大 聲 一 點 兒 。 EOS\n",
            "請講更多的聲外。\n",
            "\n",
            "BOS keep next sunday free . EOS\n",
            "BOS 把 下 周 日 空 出 来 。 EOS\n",
            "下個星期一下吧。\n",
            "\n",
            "BOS i made a mistake . EOS\n",
            "BOS 我 犯 了 一 個 錯 。 EOS\n",
            "我错了錯誤。\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSWKTpHwWmO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    }
  ]
}