{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "cn",
      "useGoogleTranslate": true
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "168.991px",
        "width": "201.634px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "191.094px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "398px",
        "left": "840px",
        "right": "20px",
        "top": "77px",
        "width": "505px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Bert手写版本+MLM+NSP.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/douzujun/NLP-Project/blob/master/Bert%E6%89%8B%E5%86%99%E7%89%88%E6%9C%AC%2BMLM%2BNSP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUG6EgDHHOWm"
      },
      "source": [
        "# Bert手写版本+MLM+NSP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:16:40.637738Z",
          "start_time": "2020-10-11T14:16:39.974576Z"
        },
        "id": "1xxvnLCcHOWq"
      },
      "source": [
        "import re\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "from random import *\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "import torch.optim as optim\n",
        "import torch.utils.data as Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZA_5gbPHOWz"
      },
      "source": [
        "# 数据预处理\n",
        "\n",
        "## 构造单词表和映射"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:16:40.677834Z",
          "start_time": "2020-10-11T14:16:40.670965Z"
        },
        "code_folding": [],
        "id": "xGnYUQDkHOW1"
      },
      "source": [
        "text = (\n",
        "    'Hello, how are you? I am Romeo.\\n'                   # R\n",
        "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n' # J\n",
        "    'Nice to meet you too. How are you today?\\n'          # R\n",
        "    'Great. My baseball team won the competition.\\n'      # J\n",
        "    'Oh Congratulations, Juliet\\n'                        # R\n",
        "    'Thank you Romeo\\n'                                   # J\n",
        "    'Where are you going today?\\n'                        # R\n",
        "    'I am going shopping. What about you?\\n'              # J\n",
        "    'I am going to visit my grandmother. she is not very well' # R\n",
        ")\n",
        "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')    # filter '.', ',', '?', '!'\n",
        "\n",
        "# 所有句子的单词list\n",
        "word_list = list(set(\" \".join(sentences).split()))               # ['hello', 'how', 'are', 'you',...]\n",
        "\n",
        "# 给单词表中所有单词设置序号\n",
        "word2idx = {'[PAD]' : 0, '[CLS]' : 1, '[SEP]' : 2, '[MASK]' : 3}\n",
        "for i, w in enumerate(word_list):\n",
        "    word2idx[w] = i + 4\n",
        "\n",
        "# 用于 idx 映射回 word\n",
        "idx2word = {i: w for i, w in enumerate(word2idx)}\n",
        "vocab_size = len(word2idx)         # 40\n",
        "\n",
        "# token: 就是每个单词在词表中的index\n",
        "token_list = list()                # token_list存储了每一句的token\n",
        "for sentence in sentences:\n",
        "    arr = [word2idx[s] for s in sentence.split()]\n",
        "    token_list.append(arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:16:40.718513Z",
          "start_time": "2020-10-11T14:16:40.712136Z"
        },
        "lang": "en",
        "id": "5SfmKM2YHOXA",
        "outputId": "266df375-d9d6-4feb-fee3-64cb3b301564"
      },
      "source": [
        "print(sentences[1])   # hello romeo my name is juliet nice to meet you\n",
        "print(token_list[1])  # [14, 31, 35, 33, 27, 11, 8, 16, 5, 34]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello romeo my name is juliet nice to meet you\n",
            "[38, 14, 23, 15, 24, 30, 5, 13, 39, 19]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRXMsY82HOXM"
      },
      "source": [
        "## 设置超参数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:16:41.118824Z",
          "start_time": "2020-10-11T14:16:41.113460Z"
        },
        "id": "XAGo9iSDHOXN"
      },
      "source": [
        "maxlen = 30      # 句子pad到的最大长度，即下面句子中的seq_len\n",
        "batch_size = 6 \n",
        "\n",
        "max_pred = 5     # max tokens of prediction\n",
        "n_layers = 6     # Bert中Transformer的层数\n",
        "n_heads = 12     # Multi-head的数量\n",
        "d_model = 768    # 即embedding_dim\n",
        "d_ff = 768*4     # 4*d_model, FeedForward dimension\n",
        "d_k = d_v = 64   # dimension of K(=Q), V，是d_model分割成n_heads之后的长度, 768 // 12 = 64\n",
        "\n",
        "n_segments = 2   # 分隔句子数"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYnGPYp2HOXU"
      },
      "source": [
        "# 实现Dataloader\n",
        "\n",
        "## 生成data\n",
        "\n",
        "- 选中语料中所有词的**15%**进行随机mask\n",
        "\n",
        "- 在确定要Mask掉的单词之后：\n",
        "\n",
        "  - 选中的单词，在80%的概率下被用 [MASK] 来代替\n",
        " \n",
        "  - 选中的单词，在10%的概率下不做mask，用任意非标记词代替\n",
        " \n",
        "  - 选中的单词，在10%的概率下不做mask，仍然保留原来真实的词"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:16:41.796706Z",
          "start_time": "2020-10-11T14:16:41.777687Z"
        },
        "id": "iI0sULeoHOXW"
      },
      "source": [
        "# sample IsNext and NotNext to be same in small batch size\n",
        "def make_data():\n",
        "    batch = []\n",
        "    positive = negative = 0\n",
        "    while (positive != batch_size / 2) or (negative != batch_size / 2):\n",
        "        # ==========================BERT 的 input 表示================================\n",
        "        # 随机取两个句子的index\n",
        "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences)) # sample random index in sentences\n",
        "        # 随机取两个句子\n",
        "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
        "        # Token (没有使用word piece): 单词在词典中的编码 \n",
        "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
        "        # Segment: 区分两个句子的编码（上句全为0 (CLS~SEP)，下句全为1）\n",
        "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
        "        \n",
        "        # ========================== MASK LM ==========================================\n",
        "        n_pred = min(max_pred, max(1, int(len(input_ids) * 0.15)))                        # 15 % of tokens in one sentence\n",
        "        # token在 input_ids 中的下标(不包括[CLS], [SEP])\n",
        "        cand_maked_pos = [i for i, token in enumerate(input_ids) \n",
        "                          if token != word2idx['[CLS]'] and token != word2idx['[SEP]']]  # candidate masked position\n",
        "        shuffle(cand_maked_pos)\n",
        "        \n",
        "        masked_tokens, masked_pos = [], []     # 被mask的tokens，被mask的tokens的索引号\n",
        "        for pos in cand_maked_pos[:n_pred]:   #  随机mask 15% 的tokens\n",
        "            masked_pos.append(pos)\n",
        "            masked_tokens.append(input_ids[pos])\n",
        "            # 选定要mask的词\n",
        "            if random() < 0.8:                           # 80%：被真实mask\n",
        "                input_ids[pos] = word2idx['[MASK]']\n",
        "            elif random() > 0.9:                        # 10%\n",
        "                index = randint(0, vocab_size - 1)      # random index in vocabulary\n",
        "                while index < 4:                       # 不能是 [PAD], [CLS], [SEP], [MASK]\n",
        "                    index = randint(0, vocab_size - 1)\n",
        "                input_ids[pos] = index                 # 10%：不做mask，用任意非标记词代替\n",
        "            # 还有10%：不做mask，什么也不做\n",
        "            \n",
        "        # =========================== Paddings ========================================\n",
        "        # input_ids全部padding到相同的长度\n",
        "        n_pad = maxlen - len(input_ids)\n",
        "        input_ids.extend([word2idx['[PAD]']] * n_pad)\n",
        "        segment_ids.extend([word2idx['[PAD]']] * n_pad)\n",
        "            \n",
        "        # zero padding (100% - 15%) tokens\n",
        "        if max_pred > n_pred:\n",
        "            n_pad = max_pred - n_pred\n",
        "            masked_tokens.extend([0] * n_pad)\n",
        "            masked_pos.extend([0] * n_pad)\n",
        "        \n",
        "        # =====================batch添加数据, 让正例 和 负例 数量相同=======================\n",
        "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])  # IsNext\n",
        "            positive += 1\n",
        "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size / 2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
        "            negative += 1\n",
        "        \n",
        "    return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T07:12:34.813381Z",
          "start_time": "2020-10-11T07:12:34.807625Z"
        },
        "id": "8TdXloLHHOXd"
      },
      "source": [
        "调用上面函数："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:16:41.987896Z",
          "start_time": "2020-10-11T14:16:41.980913Z"
        },
        "id": "bptQwzl7HOXh",
        "outputId": "fd09cb98-b360-454c-9a2d-45922f91a25f"
      },
      "source": [
        "batch = make_data()\n",
        "\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = zip(*batch)  \n",
        "print(len(isNext))\n",
        "# # 全部要转成LongTensor类型\n",
        "# input_ids, segment_ids, masked_tokens, masked_pos, isNext = \\\n",
        "#     torch.LongTensor(input_ids), torch.LongTensor(segment_ids), torch.LongTensor(masked_tokens), \\\n",
        "#     torch.LongTensor(masked_pos), torch.LongTensor(isNext)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nZBtL7dHOXx"
      },
      "source": [
        "## 生成DataLoader\n",
        "\n",
        "- 为了使用dataloader，我们需要定义以下两个function:\n",
        "\n",
        "  - `__len__` function：需要返回整个数据集中有多少个item\n",
        "  \n",
        "  - `__get__ `：根据给定的index返回一个item\n",
        "  \n",
        "有了dataloader之后，我们可以轻松随机打乱整个数据集，拿到一个batch的数据等等。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:16:42.699706Z",
          "start_time": "2020-10-11T14:16:42.689736Z"
        },
        "id": "SrcLTns2HOX2"
      },
      "source": [
        "class MyDataSet(Data.Dataset):\n",
        "    def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, isNext):\n",
        "        # 全部要转成LongTensor类型\n",
        "        self.input_ids = torch.LongTensor(input_ids)\n",
        "        self.segment_ids = torch.LongTensor(segment_ids)\n",
        "        self.masked_tokens = torch.LongTensor(masked_tokens) \n",
        "        self.masked_pos = torch.LongTensor(masked_pos) \n",
        "        self.isNext = torch.LongTensor(isNext)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.segment_ids[idx], self.masked_tokens[idx], self.masked_pos[idx], self.isNext[idx]\n",
        "    \n",
        "dataset = MyDataSet(input_ids, segment_ids, masked_tokens, masked_pos, isNext)\n",
        "dataloader = Data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:16:42.946111Z",
          "start_time": "2020-10-11T14:16:42.932823Z"
        },
        "scrolled": false,
        "id": "4OpB-jmyHOYC",
        "outputId": "82008c10-e44d-4636-c45e-08870a72ff5d"
      },
      "source": [
        "print(next(iter(dataloader)))\n",
        "print(len(dataloader))           # 就一个batch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([[ 1, 36, 23,  9, 16, 33,  3, 18,  2, 31, 21, 30,  2,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1, 36, 23,  9, 16, 33,  3, 18,  2, 22,  8,  6, 13, 28, 23, 34,  3, 24,\n",
            "         11, 27, 37,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1, 22,  8,  6,  3, 35, 12, 19,  2,  5, 13, 39, 19, 10, 25, 26, 19, 17,\n",
            "          2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1, 38, 14, 23, 15, 24, 30,  5, 13, 39, 19,  2, 38, 14, 23, 15, 24, 30,\n",
            "          5, 13,  3, 19,  2,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1, 29, 26, 19,  6,  3,  2, 22,  8,  6, 32, 35,  3, 19,  2,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1, 38, 14, 23, 15, 24, 30,  5, 13, 39, 19,  2,  5, 13, 39, 19, 10, 25,\n",
            "          3, 19, 17,  2,  0,  0,  0,  0,  0,  0,  0,  0]]), tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "         0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0]]), tensor([[20,  0,  0,  0,  0],\n",
            "        [ 4, 20, 23,  0,  0],\n",
            "        [12, 32,  0,  0,  0],\n",
            "        [14, 14, 39,  0,  0],\n",
            "        [17, 12,  0,  0,  0],\n",
            "        [17, 39, 26,  0,  0]]), tensor([[ 6,  0,  0,  0,  0],\n",
            "        [16,  6, 14,  0,  0],\n",
            "        [ 6,  4,  0,  0,  0],\n",
            "        [ 2, 13, 20,  0,  0],\n",
            "        [ 5, 12,  0,  0,  0],\n",
            "        [20,  9, 18,  0,  0]]), tensor([1, 0, 0, 0, 1, 1])]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T11:40:34.174618Z",
          "start_time": "2020-10-11T11:40:34.168686Z"
        },
        "id": "vci2w21zHOYO"
      },
      "source": [
        "# Bert模型\n",
        "\n",
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:16:43.711528Z",
          "start_time": "2020-10-11T14:16:43.699319Z"
        },
        "id": "zLE6v5d5HOYT"
      },
      "source": [
        "class BertEmbedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertEmbedding, self).__init__()\n",
        "        # d_model:即embedding_dim\n",
        "        # token embedding\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model)  \n",
        "\n",
        "        # position embedding: 这里简写了,源码中位置编码使用了sin，cos\n",
        "#         self.pos_embed = nn.Embedding(maxlen, d_model)      \n",
        "        self.pos_embed = torch.tensor(\n",
        "            [[pos / (10000.0 ** (i // 2 * 2.0 / d_model)) for i in range(d_model)] for pos in range(maxlen)]\n",
        "        )\n",
        "        self.pos_embed[:, 0::2] = torch.sin(self.pos_embed[:, 0::2])\n",
        "        self.pos_embed[:, 1::2] = torch.cos(self.pos_embed[:, 1::2])\n",
        "        \n",
        "        # segment embedding\n",
        "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
        "\n",
        "        # LayerNorm\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        \n",
        "    def forward(self, x, seq):                  # x 和 pos的shape 都是[batch_size, seq_len]\n",
        "\n",
        "#         seq_len = x.size(1)        \n",
        "#         pos = torch.arange(seq_len, dtype=torch.long)\n",
        "        # unsqueeze(0): 在索引0处，增加维度--> [1, seq_len]\n",
        "        # expand: 某个 size=1 的维度上扩展到size\n",
        "        # expand_as: 把一个tensor变成和函数括号内一样形状的tensor\n",
        "#         pos = pos.unsqueeze(0).expand_as(x)     # [seq_len] -> [batch_size, seq_len]\n",
        "    \n",
        "        # 三个embedding相加\n",
        "        input_embedding = self.tok_embed(x) + nn.Parameter(self.pos_embed, requires_grad=False) + self.seg_embed(seq)\n",
        "        \n",
        "        return self.norm(input_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T12:53:38.450646Z",
          "start_time": "2020-10-11T12:53:38.427512Z"
        },
        "id": "mMIuvYhCHOYh"
      },
      "source": [
        "## 生成mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:16:44.304800Z",
          "start_time": "2020-10-11T14:16:44.299614Z"
        },
        "id": "saCDe7HPHOYw"
      },
      "source": [
        "# Padding的部分不应该计算概率，所以需要在相应位置设置mask\n",
        "# mask==0的内容填充1e-9，使得计算softmax时概率接近0\n",
        "# 在计算attention时，使用\n",
        "def get_attn_pad_mask(seq_q, seq_k):    # seq_q 和 seq_k 的 shape 都是 [batch_size, seq_len]\n",
        "    batch_size, seq_len = seq_q.size()\n",
        "    # eq(zero) is PAD token\n",
        "    pad_attn_mask = seq_q.data.eq(0).unsqueeze(1)              # [batcb_size, 1, seq_len]\n",
        "    return pad_attn_mask.expand(batch_size, seq_len, seq_len) # [batch_size, seq_len, seq_len]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaYxEVEoHOY8"
      },
      "source": [
        "## 构建激活函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:16:45.236139Z",
          "start_time": "2020-10-11T14:16:45.231847Z"
        },
        "id": "lButqpZMHOY-"
      },
      "source": [
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lNlLYXHHOZF"
      },
      "source": [
        "## 缩放点乘注意力计算\n",
        "\n",
        "- $self-att(Q,K,V) = V \\cdot softmax(\\frac{K^T \\cdot Q}{\\sqrt{D_k}}$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:16:46.062685Z",
          "start_time": "2020-10-11T14:16:46.053866Z"
        },
        "id": "zRBsXE3vHOZH"
      },
      "source": [
        " class ScaledDotProductAttention(nn.Module): \n",
        "        \"\"\"\n",
        "        Scaled Dot-Product Attention\n",
        "        \"\"\"\n",
        "        def __init__(self):\n",
        "            super(ScaledDotProductAttention, self).__init__()\n",
        "            \n",
        "        def forward(self, Q, K, V, attn_mask):\n",
        "            \"\"\"\n",
        "            Args:\n",
        "                Q: [batch_size, n_heads, seq_len, d_k]\n",
        "                K: [batch_size, n_heads, seq_len, d_k]\n",
        "                V: [batch_size, n_heads, seq_len, d_k]\n",
        "            Return:\n",
        "                self-attention后的张量，以及attention张量\n",
        "            \"\"\"\n",
        "            # [batch_size, n_heads, seq_len, d_k] * [batch_size, n_heads, d_k, seq_len] = [batch_size, n_heads, seq_len, seq_len]\n",
        "            score = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
        "            \n",
        "            # mask==0 is PAD token\n",
        "            score = score.masked_fill_(attn_mask, -1e9) # mask==0的内容填充1e-9，使得计算softmax时概率接近0\n",
        "                \n",
        "            attention = F.softmax(score, dim = -1)          # [bz, n_hs, seq_len, seq_len]\n",
        "            context = torch.matmul(attention, V)            # [batch_size, n_heads, seq_len, d_k]\n",
        "            \n",
        "            return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlRcTyfzHOZQ"
      },
      "source": [
        "## Multi-Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:16:47.009304Z",
          "start_time": "2020-10-11T14:16:46.994583Z"
        },
        "id": "-8xKRvdBHOZS"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)   # 其实就是[d_model, d_model]\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):             # Q和K: [batch_size, seq_len, d_model], V: [batch_size, seq_len, d_model], attn_mask: [batch_size, seq_len, seq_len]\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size, n_heads, seq_len, d_k]\n",
        "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size, n_heads, seq_len, d_k]\n",
        "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size, n_heads, seq_len, d_v]\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)          # attn_mask : [batch_size, n_heads, seq_len, seq_len]\n",
        "\n",
        "        # context: [batch_size, n_heads, seq_len, d_v], attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
        "        context = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size, seq_len, n_heads, d_v]\n",
        "        \n",
        "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
        "       \n",
        "        return nn.LayerNorm(d_model)(output + residual)                      # output: [batch_size, seq_len, d_model]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppWRazRTHOZY"
      },
      "source": [
        "## 前向传播\n",
        "\n",
        "- Position_wise_Feed_Forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:16:47.874045Z",
          "start_time": "2020-10-11T14:16:47.867372Z"
        },
        "id": "35A60BrcHOZa"
      },
      "source": [
        "class PoswiseFeedForwardNet(nn.Module):        # 前向传播，线性激活再线性\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # [batch_size, seq_len, d_model] -> [batch_size, seq_len, d_ff] -> [batch_size, seq_len, d_model]\n",
        "        return self.fc2(gelu(self.fc1(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcC2rYvmHOZj"
      },
      "source": [
        "## 编码层EncoderLayer\n",
        "\n",
        "源码中 `Bidirectional Encoder = Transformer (self-attention)`\n",
        "\n",
        "`Transformer = MultiHead_Attention + Feed_Forward with sublayer connection`，下面代码省去了sublayer。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:16:48.545383Z",
          "start_time": "2020-10-11T14:16:48.538822Z"
        },
        "id": "Ut0dr87YHOZl"
      },
      "source": [
        "class EncoderLayer(nn.Module):    #多头注意力和前向传播的组合\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
        "        enc_outputs = self.pos_ffn(enc_outputs)             # enc_outputs: [batch_size, seq_len, d_model]\n",
        "        return enc_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fisjyyYVHOZx"
      },
      "source": [
        "## BERT模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:16:49.113277Z",
          "start_time": "2020-10-11T14:16:49.098177Z"
        },
        "id": "zEmwiYC0HOZz"
      },
      "source": [
        "class BERT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "        self.embedding = BertEmbedding()\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "        self.activ2 = gelu\n",
        "        # fc2 is shared with embedding layer\n",
        "        embed_weight = self.embedding.tok_embed.weight         \n",
        "        self.fc2 = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.fc2.weight = embed_weight\n",
        "\n",
        "    # input_ids和segment_ids的shape[batch_size, seq_len]，masked_pos的shape是[batch_size, max_pred]\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):          \n",
        "        output = self.embedding(input_ids, segment_ids)             # [bach_size, seq_len, d_model]\n",
        "\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)# [batch_size, seq_len, seq_len]\n",
        "        for layer in self.layers:                                  # 这里对layers遍历，相当于源码中多个transformer_blocks\n",
        "            output = layer(output, enc_self_attn_mask)              # output: [batch_size, seq_len, d_model]\n",
        "\n",
        "        # it will be decided by first token(CLS)\n",
        "        h_pooled = self.fc(output[:, 0])                   # [batch_size, d_model]\n",
        "        logits_clsf = self.classifier(h_pooled)            # [batch_size, 2] predict isNext\n",
        "\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, d_model) # [batch_size, max_pred, d_model]\n",
        "        h_masked = torch.gather(output, 1, masked_pos)              # masking position [batch_size, max_pred, d_model]\n",
        "        h_masked = self.activ2(self.linear(h_masked))               # [batch_size, max_pred, d_model]\n",
        "        logits_lm = self.fc2(h_masked)                              # [batch_size, max_pred, vocab_size]\n",
        "        \n",
        "        # logits_lm: [batch_size, max_pred, vocab_size], logits_clsf: [batch_size, 2]\n",
        "        return logits_lm, logits_clsf                     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU01ritBHOZ7"
      },
      "source": [
        "## 定义模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:16:50.299667Z",
          "start_time": "2020-10-11T14:16:49.733700Z"
        },
        "id": "T08whxgSHOZ8"
      },
      "source": [
        "model = BERT()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psNxVxzQHOaF"
      },
      "source": [
        "# 训练模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:17:14.367211Z",
          "start_time": "2020-10-11T14:16:50.501600Z"
        },
        "id": "jIeAVzETHOaG",
        "outputId": "8de8420e-1b36-43d1-f976-25cb1f9f35de"
      },
      "source": [
        "for epoch in range(50):\n",
        "    for input_ids, segment_ids, masked_tokens, masked_pos, isNext in dataloader:\n",
        "        \n",
        "        # logits_lm: [batch_size, max_pred, vocab_size]\n",
        "        # logits_clsf: [batch_size, 2]\n",
        "        logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)          \n",
        "        \n",
        "        loss_lm = criterion(logits_lm.view(-1, vocab_size), masked_tokens.view(-1)) # for masked LM\n",
        "        loss_lm = (loss_lm.float()).mean()\n",
        "        \n",
        "        loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
        "        loss = loss_lm + loss_clsf\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0010 loss = 1.908749\n",
            "Epoch: 0020 loss = 1.354349\n",
            "Epoch: 0030 loss = 1.131212\n",
            "Epoch: 0040 loss = 1.091269\n",
            "Epoch: 0050 loss = 0.891469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-veXTwwHOaM"
      },
      "source": [
        "# 预测"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:19:14.300430Z",
          "start_time": "2020-10-11T14:19:14.294011Z"
        },
        "id": "K8LHxRe6HOaN",
        "outputId": "090a7028-9df7-4b05-d18a-7013c47700d0"
      },
      "source": [
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = batch[1]\n",
        "print(text)\n",
        "print('================================')\n",
        "print([idx2word[w] for w in input_ids if idx2word[w] != '[PAD]'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello, how are you? I am Romeo.\n",
            "Hello, Romeo My name is Juliet. Nice to meet you.\n",
            "Nice to meet you too. How are you today?\n",
            "Great. My baseball team won the competition.\n",
            "Oh Congratulations, Juliet\n",
            "Thank you Romeo\n",
            "Where are you going today?\n",
            "I am going shopping. What about you?\n",
            "I am going to visit my grandmother. she is not very well\n",
            "================================\n",
            "['[CLS]', 'great', 'my', 'baseball', 'team', 'won', '[MASK]', 'competition', '[SEP]', 'i', 'am', 'going', 'to', 'visit', 'my', 'grandmother', '[MASK]', 'is', 'not', 'very', 'well', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:36:53.929639Z",
          "start_time": "2020-10-11T14:36:53.831314Z"
        },
        "id": "p5hYK0CAHOaW",
        "outputId": "debd5cb6-9c45-4e93-8dda-1a3d11e66d03"
      },
      "source": [
        "logits_lm, logits_clsf = model(torch.LongTensor([input_ids]), torch.LongTensor([segment_ids]), \n",
        "                               torch.LongTensor([masked_pos])) # batch=1\n",
        "# vocab_size维上求max, 输出最大值的索引，第一个batch的max_pred个输出\n",
        "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
        "print('masked tokens list: ', [pos for pos in masked_tokens if pos != 0])\n",
        "print('predict masked tokens list: ', [pos for pos in logits_lm if pos != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "masked tokens list:  [4, 20, 23]\n",
            "predict masked tokens list:  [26, 20, 23]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-11T14:37:00.270089Z",
          "start_time": "2020-10-11T14:37:00.262266Z"
        },
        "id": "crYtY_iiHOah",
        "outputId": "e329e8ca-4248-459d-90df-ea168743835d"
      },
      "source": [
        "pred = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
        "print('isNext : ', True if isNext else False)\n",
        "print('predict isNext ：', True if pred else False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "isNext :  False\n",
            "predict isNext ： False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw3ZYr8PHOas"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTDAED7mHOa2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBbBrn4wHObD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcWaUF_jHObM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67nTNfB1HObV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}