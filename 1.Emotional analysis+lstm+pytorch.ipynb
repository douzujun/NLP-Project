{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPPNzC0+L5ZLvET9UfvEjd1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/douzujun/NLP-Project/blob/master/LSTM-Emotional%20analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X9yjs5_7z4p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "outputId": "cb78ac54-4c37-4fff-ee10-6780fba2a77c"
      },
      "source": [
        "!pip install torch\n",
        "!pip install torchtext\n",
        "!python -m spacy download en\n",
        "!pip install torchvision\n",
        "\n",
        "# K80 gpu for 12 hours\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchtext import data, datasets\n",
        "\n",
        "print('GPU:', torch.cuda.is_available())\n",
        "\n",
        "torch.manual_seed(123)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.1+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.5.1+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (49.1.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.6.1+cu101)\n",
            "Requirement already satisfied: torch==1.5.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.5.1+cu101)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1->torchvision) (0.16.0)\n",
            "GPU: True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f45002e3130>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HovvE-c8BeQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "55007511-700b-400f-890b-afacefedf08f"
      },
      "source": [
        "# 为CPU设置随机种子\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 两个Field对象定义字段的处理方法（文本字段、标签字段）\n",
        "TEXT = data.Field(tokenize='spacy')  # 分词\n",
        "LABEL = data.LabelField(dtype=torch.float)\n",
        "\n",
        "# IMDB共50000影评，包含正面和负面两个类别。数据被前面的Field处理\n",
        "# 按照(TEXT, LABEL) 分割成 训练集，测试集\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "print('len of train data:', len(train_data))        # 25000\n",
        "print('len of test data:', len(test_data))          # 25000\n",
        "\n",
        "# torchtext.data.Example : 用来表示一个样本，数据+标签\n",
        "print(train_data.examples[15].text)                 # 文本：句子的单词列表\n",
        "print(train_data.examples[15].label)                # 标签: 积极"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:08<00:00, 10.0MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "len of train data: 25000\n",
            "len of test data: 25000\n",
            "['The', 'movie', 'is', 'a', 'bit', '\"', 'thin', '\"', 'after', 'reading', 'the', 'book', ',', 'but', 'it', \"'s\", 'still', 'one', 'of', 'the', 'greatest', 'movies', 'ever', 'made', '.', 'Sheryl', 'Lee', 'is', 'beautiful', 'and', 'Nick', 'Nolte', 'is', 'really', '\"', 'vonneguty', '\"', '.', 'He', 'makes', 'great', 'job', 'expressing', 'the', 'feelings', 'from', 'the', 'book', 'to', 'the', 'film', '.', 'Not', 'many', 'films', 'engage', 'the', 'feeling', 'of', 'the', 'book', 'as', 'well', 'as', 'Mother', 'Night', 'does', '.']\n",
            "pos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbKax6528BuX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "994a91d4-891f-4082-f9ba-b98026572c22"
      },
      "source": [
        "TEXT.build_vocab(train_data, max_size=10000, vectors='glove.6B.100d')\n",
        "LABEL.build_vocab(train_data)\n",
        "print(len(TEXT.vocab))           # 10002\n",
        "print(TEXT.vocab.itos[:12])        # ['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is', 'in', 'I']\n",
        "print(TEXT.vocab.stoi['and'])       # 5\n",
        "print(LABEL.vocab.stoi)          # defaultdict(None, {'neg': 0, 'pos': 1})"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:30, 2.21MB/s]                           \n",
            "100%|█████████▉| 398894/400000 [00:16<00:00, 23519.32it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10002\n",
            "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is', 'in', 'I']\n",
            "5\n",
            "defaultdict(<function _default_unk_index at 0x7f44ffd3ed90>, {'neg': 0, 'pos': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfv4g5Si8B7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batchsz = 30\n",
        "device = torch.device('cuda')\n",
        "train_iterator, test_iterator = data.BucketIterator.splits(\n",
        "                                (train_data, test_data),\n",
        "                                batch_size = batchsz,\n",
        "                                device=device\n",
        "                               )"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei2LDRY18B4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "    super(RNN, self).__init__()\n",
        "\n",
        "    # [0-10001] => [100]\n",
        "    # 参数1:embedding个数(单词数), 参数2:embedding的维度(词向量维度)\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    # [100] => [256]\n",
        "    # 双向LSTM，所以下面FC层使用 hidden_dim*2\n",
        "    self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=2,\n",
        "                       bidirectional=True, dropout=0.5) \n",
        "    # [256*2] => [1]\n",
        "    self.fc = nn.Linear(hidden_dim*2, 1)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    x: [seq_len, b] vs [b, 3, 28, 28]\n",
        "    \"\"\"\n",
        "    # [seq_len, b, 1] => [seq_len, b, 100]\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "    # output: [seq, b, hid_dim*2]\n",
        "    # hidden/h: [num_layers*2, b, hid_dim]\n",
        "    # cell/c: [num_layers*2, b, hid_dim]\n",
        "    output, (hidden, cell) = self.rnn(embedding)\n",
        "    # [num_layers*2, b, hid_dim] => 2 of [b, hid_dim] => [b, hid_dim*2]\n",
        "    # 双向，所以要把最后两个输出连接\n",
        "    hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
        "    # [b, hid_dim*2] => [b, 1]\n",
        "    hidden = self.dropout(hidden)\n",
        "    out = self.fc(hidden)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb7WYVEmCYqG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a2ebe150-3aff-4880-a4bc-06f1ada66856"
      },
      "source": [
        "rnn = RNN(len(TEXT.vocab), 100, 256)                          #词个数，词嵌入维度，输出维度\n",
        "\n",
        "pretrained_embedding = TEXT.vocab.vectors\n",
        "print('pretrained_embedding:', pretrained_embedding.shape)    # torch.Size([10002, 100])\n",
        "\n",
        "# 使用预训练过的embedding来替换随机初始化\n",
        "rnn.embedding.weight.data.copy_(pretrained_embedding)\n",
        "print('embedding layer inited.')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pretrained_embedding: torch.Size([10002, 100])\n",
            "embedding layer inited.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_OIamPtJP3L",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbnpMTyhCYny",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "outputId": "8342c342-2c54-488d-e780-eb882657cdac"
      },
      "source": [
        "optimizer = optim.Adam(rnn.parameters(), lr=1e-3)\n",
        "# BCEWithLogitsLoss是针对二分类的CrossEntropy\n",
        "criteon = nn.BCEWithLogitsLoss().to(device)\n",
        "rnn.to(device)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(10002, 100)\n",
              "  (rnn): LSTM(100, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UXwTad6CYlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "def binary_acc(preds, y):\n",
        "\n",
        "  preds = torch.round(torch.sigmoid(preds))\n",
        "  correct = torch.eq(preds, y).float()\n",
        "  acc = correct.sum() / len(correct)\n",
        "  return acc"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXMx5OVwCYjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(rnn, iterator, optimizer, criteon):\n",
        "  avg_acc = []\n",
        "  rnn.train()   # 表示进入训练模式\n",
        "\n",
        "  for i, batch in enumerate(iterator):\n",
        "    # [seq, b] => [b, 1] => [b]\n",
        "    # batch.text 就是上面forward函数的参数text，压缩维度是为了和batch.label维度一致\n",
        "    pred = rnn(batch.text).squeeze(1)\n",
        "\n",
        "    loss = criteon(pred, batch.label)\n",
        "    # 计算每个batch的准确率\n",
        "    acc = binary_acc(pred, batch.label).item()\n",
        "    avg_acc.append(acc)\n",
        "\n",
        "    optimizer.zero_grad() # 清零梯度准备计算\n",
        "    loss.backward()    # 反向传播\n",
        "    optimizer.step()   # 更新训练参数\n",
        "\n",
        "    if i % 10 == 0:\n",
        "      print(i, acc)\n",
        "\n",
        "  avg_acc = np.array(avg_acc).mean()\n",
        "  print('avg acc:', avg_acc)\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRhMMq0rCYg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(rnn, iterator, criteon):\n",
        "  avg_acc = []\n",
        "  rnn.eval()         # 表示进入测试模式\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in iterator:\n",
        "      pred = rnn(batch.text).squeeze(1)      # [b, 1] => [b]\n",
        "      loss = criteon(pred, batch.label)\n",
        "      acc = binary_acc(pred, batch.label).item()\n",
        "      avg_acc.append(acc)\n",
        "\n",
        "  avg_acc = np.array(avg_acc).mean()\n",
        "\n",
        "  print('test acc:', avg_acc)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diKF7CfZCYd4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c356ec4-70b9-4a6f-88b9-23172fa0df0b"
      },
      "source": [
        "for epoch in range(10):\n",
        "    \n",
        "  train(rnn, train_iterator, optimizer, criteon)\n",
        "  \n",
        "  evaluate(rnn, test_iterator, criteon)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.8666667342185974\n",
            "10 0.9666666984558105\n",
            "20 0.8000000715255737\n",
            "30 0.8666667342185974\n",
            "40 0.8666667342185974\n",
            "50 0.8000000715255737\n",
            "60 0.9333333969116211\n",
            "70 0.7666667103767395\n",
            "80 0.9000000357627869\n",
            "90 0.8666667342185974\n",
            "100 0.9000000357627869\n",
            "110 0.7666667103767395\n",
            "120 0.8000000715255737\n",
            "130 0.9666666984558105\n",
            "140 0.8666667342185974\n",
            "150 0.9000000357627869\n",
            "160 0.9000000357627869\n",
            "170 0.9000000357627869\n",
            "180 0.8000000715255737\n",
            "190 0.8000000715255737\n",
            "200 0.9333333969116211\n",
            "210 0.9000000357627869\n",
            "220 0.9333333969116211\n",
            "230 0.8666667342185974\n",
            "240 0.9000000357627869\n",
            "250 0.7666667103767395\n",
            "260 0.9333333969116211\n",
            "270 0.9000000357627869\n",
            "280 0.8000000715255737\n",
            "290 0.8666667342185974\n",
            "300 0.9333333969116211\n",
            "310 0.7666667103767395\n",
            "320 0.9000000357627869\n",
            "330 0.9666666984558105\n",
            "340 0.9666666984558105\n",
            "350 0.8333333730697632\n",
            "360 0.9000000357627869\n",
            "370 0.8000000715255737\n",
            "380 0.9000000357627869\n",
            "390 0.8666667342185974\n",
            "400 0.8333333730697632\n",
            "410 0.9000000357627869\n",
            "420 0.9333333969116211\n",
            "430 0.8333333730697632\n",
            "440 0.8666667342185974\n",
            "450 0.8000000715255737\n",
            "460 0.9333333969116211\n",
            "470 0.8666667342185974\n",
            "480 0.9333333969116211\n",
            "490 0.9333333969116211\n",
            "500 0.9000000357627869\n",
            "510 0.8333333730697632\n",
            "520 0.8666667342185974\n",
            "530 0.9333333969116211\n",
            "540 0.9333333969116211\n",
            "550 0.7666667103767395\n",
            "560 0.8333333730697632\n",
            "570 0.9333333969116211\n",
            "580 0.9000000357627869\n",
            "590 0.9333333969116211\n",
            "600 0.9000000357627869\n",
            "610 0.8333333730697632\n",
            "620 0.7333333492279053\n",
            "630 0.8333333730697632\n",
            "640 0.8333333730697632\n",
            "650 0.9000000357627869\n",
            "660 0.9333333969116211\n",
            "670 0.8000000715255737\n",
            "680 0.9000000357627869\n",
            "690 0.9000000357627869\n",
            "700 0.9000000357627869\n",
            "710 0.9333333969116211\n",
            "720 0.8000000715255737\n",
            "730 0.9333333969116211\n",
            "740 0.9666666984558105\n",
            "750 0.9666666984558105\n",
            "760 0.9333333969116211\n",
            "770 0.8666667342185974\n",
            "780 0.8666667342185974\n",
            "790 0.8666667342185974\n",
            "800 0.9666666984558105\n",
            "810 0.9000000357627869\n",
            "820 0.9000000357627869\n",
            "830 0.9333333969116211\n",
            "avg acc: 0.8855715916454078\n",
            "test acc: 0.8775779855051201\n",
            "0 0.9000000357627869\n",
            "10 0.9666666984558105\n",
            "20 0.9000000357627869\n",
            "30 0.9000000357627869\n",
            "40 0.9666666984558105\n",
            "50 0.9666666984558105\n",
            "60 0.7666667103767395\n",
            "70 0.8666667342185974\n",
            "80 0.9333333969116211\n",
            "90 0.9000000357627869\n",
            "100 0.9333333969116211\n",
            "110 0.8666667342185974\n",
            "120 0.9000000357627869\n",
            "130 0.9000000357627869\n",
            "140 0.8666667342185974\n",
            "150 0.8333333730697632\n",
            "160 0.8333333730697632\n",
            "170 0.9333333969116211\n",
            "180 0.8333333730697632\n",
            "190 0.9000000357627869\n",
            "200 0.8666667342185974\n",
            "210 1.0\n",
            "220 1.0\n",
            "230 0.9666666984558105\n",
            "240 0.9000000357627869\n",
            "250 0.8000000715255737\n",
            "260 0.9333333969116211\n",
            "270 0.9666666984558105\n",
            "280 0.9333333969116211\n",
            "290 0.9666666984558105\n",
            "300 0.9000000357627869\n",
            "310 0.9333333969116211\n",
            "320 0.9333333969116211\n",
            "330 0.9666666984558105\n",
            "340 0.9666666984558105\n",
            "350 0.9666666984558105\n",
            "360 0.9333333969116211\n",
            "370 0.9666666984558105\n",
            "380 0.8333333730697632\n",
            "390 0.7333333492279053\n",
            "400 0.9000000357627869\n",
            "410 0.9000000357627869\n",
            "420 0.8000000715255737\n",
            "430 0.9333333969116211\n",
            "440 0.8666667342185974\n",
            "450 0.9333333969116211\n",
            "460 0.8333333730697632\n",
            "470 0.9333333969116211\n",
            "480 0.9333333969116211\n",
            "490 0.8000000715255737\n",
            "500 0.9666666984558105\n",
            "510 0.9000000357627869\n",
            "520 1.0\n",
            "530 0.9666666984558105\n",
            "540 1.0\n",
            "550 0.9333333969116211\n",
            "560 0.9000000357627869\n",
            "570 1.0\n",
            "580 0.9000000357627869\n",
            "590 0.9000000357627869\n",
            "600 0.8666667342185974\n",
            "610 0.8333333730697632\n",
            "620 0.9000000357627869\n",
            "630 0.9000000357627869\n",
            "640 0.8666667342185974\n",
            "650 0.9000000357627869\n",
            "660 0.9666666984558105\n",
            "670 0.9333333969116211\n",
            "680 0.8666667342185974\n",
            "690 0.9000000357627869\n",
            "700 0.8666667342185974\n",
            "710 0.9333333969116211\n",
            "720 0.9666666984558105\n",
            "730 0.9666666984558105\n",
            "740 0.9666666984558105\n",
            "750 0.9000000357627869\n",
            "760 0.9000000357627869\n",
            "770 0.9000000357627869\n",
            "780 0.9333333969116211\n",
            "790 0.9333333969116211\n",
            "800 0.9333333969116211\n",
            "810 0.8666667342185974\n",
            "820 0.9000000357627869\n",
            "830 0.9000000357627869\n",
            "avg acc: 0.9071942910873633\n",
            "test acc: 0.8886890964542361\n",
            "0 0.9333333969116211\n",
            "10 0.9333333969116211\n",
            "20 0.9666666984558105\n",
            "30 0.9333333969116211\n",
            "40 0.9333333969116211\n",
            "50 0.8666667342185974\n",
            "60 1.0\n",
            "70 0.8333333730697632\n",
            "80 0.9666666984558105\n",
            "90 0.9000000357627869\n",
            "100 0.9666666984558105\n",
            "110 0.9666666984558105\n",
            "120 0.9333333969116211\n",
            "130 0.9333333969116211\n",
            "140 0.9000000357627869\n",
            "150 0.9666666984558105\n",
            "160 0.8666667342185974\n",
            "170 0.9666666984558105\n",
            "180 0.9666666984558105\n",
            "190 0.9333333969116211\n",
            "200 0.9333333969116211\n",
            "210 0.8666667342185974\n",
            "220 0.9000000357627869\n",
            "230 0.8333333730697632\n",
            "240 0.9333333969116211\n",
            "250 0.8000000715255737\n",
            "260 0.8666667342185974\n",
            "270 0.9000000357627869\n",
            "280 0.9000000357627869\n",
            "290 0.9666666984558105\n",
            "300 0.9333333969116211\n",
            "310 0.9000000357627869\n",
            "320 0.9333333969116211\n",
            "330 0.9666666984558105\n",
            "340 0.9000000357627869\n",
            "350 1.0\n",
            "360 0.9666666984558105\n",
            "370 0.9333333969116211\n",
            "380 0.9333333969116211\n",
            "390 0.9666666984558105\n",
            "400 0.9666666984558105\n",
            "410 0.9666666984558105\n",
            "420 1.0\n",
            "430 0.9000000357627869\n",
            "440 1.0\n",
            "450 0.9000000357627869\n",
            "460 0.9333333969116211\n",
            "470 1.0\n",
            "480 0.9000000357627869\n",
            "490 0.9333333969116211\n",
            "500 0.9000000357627869\n",
            "510 0.9000000357627869\n",
            "520 0.9333333969116211\n",
            "530 0.9333333969116211\n",
            "540 0.9666666984558105\n",
            "550 0.9666666984558105\n",
            "560 0.9666666984558105\n",
            "570 0.9666666984558105\n",
            "580 0.8333333730697632\n",
            "590 0.9666666984558105\n",
            "600 0.9333333969116211\n",
            "610 0.9333333969116211\n",
            "620 0.9333333969116211\n",
            "630 1.0\n",
            "640 0.9000000357627869\n",
            "650 0.8666667342185974\n",
            "660 0.9333333969116211\n",
            "670 0.8666667342185974\n",
            "680 0.9666666984558105\n",
            "690 0.9333333969116211\n",
            "700 1.0\n",
            "710 0.9666666984558105\n",
            "720 0.9666666984558105\n",
            "730 0.9000000357627869\n",
            "740 0.9333333969116211\n",
            "750 0.9666666984558105\n",
            "760 1.0\n",
            "770 0.8666667342185974\n",
            "780 0.9000000357627869\n",
            "790 0.9333333969116211\n",
            "800 0.9666666984558105\n",
            "810 0.9000000357627869\n",
            "820 0.9666666984558105\n",
            "830 0.8000000715255737\n",
            "avg acc: 0.9266587171337302\n",
            "test acc: 0.8872902161068768\n",
            "0 0.9333333969116211\n",
            "10 1.0\n",
            "20 1.0\n",
            "30 0.9666666984558105\n",
            "40 0.9666666984558105\n",
            "50 1.0\n",
            "60 0.9333333969116211\n",
            "70 0.9666666984558105\n",
            "80 0.8666667342185974\n",
            "90 0.9666666984558105\n",
            "100 0.9333333969116211\n",
            "110 0.8666667342185974\n",
            "120 0.9333333969116211\n",
            "130 0.9000000357627869\n",
            "140 0.8333333730697632\n",
            "150 0.9666666984558105\n",
            "160 0.9666666984558105\n",
            "170 0.8666667342185974\n",
            "180 0.9666666984558105\n",
            "190 0.9666666984558105\n",
            "200 0.9333333969116211\n",
            "210 0.9333333969116211\n",
            "220 0.9666666984558105\n",
            "230 0.9666666984558105\n",
            "240 0.9000000357627869\n",
            "250 1.0\n",
            "260 0.9333333969116211\n",
            "270 0.9666666984558105\n",
            "280 0.9333333969116211\n",
            "290 0.9000000357627869\n",
            "300 1.0\n",
            "310 0.9333333969116211\n",
            "320 0.9666666984558105\n",
            "330 0.9666666984558105\n",
            "340 0.9333333969116211\n",
            "350 0.9333333969116211\n",
            "360 0.9333333969116211\n",
            "370 0.9333333969116211\n",
            "380 1.0\n",
            "390 1.0\n",
            "400 0.9333333969116211\n",
            "410 1.0\n",
            "420 0.9333333969116211\n",
            "430 0.9666666984558105\n",
            "440 0.9333333969116211\n",
            "450 0.9333333969116211\n",
            "460 0.9666666984558105\n",
            "470 0.8333333730697632\n",
            "480 1.0\n",
            "490 0.9333333969116211\n",
            "500 0.9666666984558105\n",
            "510 0.9000000357627869\n",
            "520 0.9000000357627869\n",
            "530 1.0\n",
            "540 0.9333333969116211\n",
            "550 0.9666666984558105\n",
            "560 0.9000000357627869\n",
            "570 0.9333333969116211\n",
            "580 0.9333333969116211\n",
            "590 0.9666666984558105\n",
            "600 0.8333333730697632\n",
            "610 0.9333333969116211\n",
            "620 0.8666667342185974\n",
            "630 0.9000000357627869\n",
            "640 0.9333333969116211\n",
            "650 0.9666666984558105\n",
            "660 0.9666666984558105\n",
            "670 0.9333333969116211\n",
            "680 0.9333333969116211\n",
            "690 0.9333333969116211\n",
            "700 0.9666666984558105\n",
            "710 0.9000000357627869\n",
            "720 0.9333333969116211\n",
            "730 1.0\n",
            "740 0.9666666984558105\n",
            "750 0.9333333969116211\n",
            "760 0.9666666984558105\n",
            "770 0.8333333730697632\n",
            "780 0.9666666984558105\n",
            "790 0.9000000357627869\n",
            "800 0.9000000357627869\n",
            "810 0.9000000357627869\n",
            "820 0.9666666984558105\n",
            "830 0.9666666984558105\n",
            "avg acc: 0.9356515197445163\n",
            "test acc: 0.890008042184569\n",
            "0 1.0\n",
            "10 1.0\n",
            "20 0.9000000357627869\n",
            "30 0.8666667342185974\n",
            "40 0.9000000357627869\n",
            "50 0.9333333969116211\n",
            "60 0.9000000357627869\n",
            "70 0.9666666984558105\n",
            "80 0.8666667342185974\n",
            "90 0.9000000357627869\n",
            "100 0.9333333969116211\n",
            "110 1.0\n",
            "120 0.9666666984558105\n",
            "130 0.9666666984558105\n",
            "140 1.0\n",
            "150 0.9333333969116211\n",
            "160 0.9333333969116211\n",
            "170 0.9333333969116211\n",
            "180 1.0\n",
            "190 0.9666666984558105\n",
            "200 0.9333333969116211\n",
            "210 1.0\n",
            "220 0.9666666984558105\n",
            "230 1.0\n",
            "240 0.9333333969116211\n",
            "250 0.8333333730697632\n",
            "260 0.9666666984558105\n",
            "270 0.9333333969116211\n",
            "280 0.9000000357627869\n",
            "290 1.0\n",
            "300 0.9666666984558105\n",
            "310 0.9333333969116211\n",
            "320 0.9000000357627869\n",
            "330 0.9000000357627869\n",
            "340 1.0\n",
            "350 0.9666666984558105\n",
            "360 1.0\n",
            "370 0.9666666984558105\n",
            "380 0.9000000357627869\n",
            "390 0.9666666984558105\n",
            "400 0.9666666984558105\n",
            "410 0.9333333969116211\n",
            "420 0.9000000357627869\n",
            "430 1.0\n",
            "440 0.9333333969116211\n",
            "450 0.9666666984558105\n",
            "460 0.9666666984558105\n",
            "470 1.0\n",
            "480 1.0\n",
            "490 0.9666666984558105\n",
            "500 1.0\n",
            "510 1.0\n",
            "520 1.0\n",
            "530 1.0\n",
            "540 0.8666667342185974\n",
            "550 1.0\n",
            "560 0.9333333969116211\n",
            "570 0.9333333969116211\n",
            "580 0.9666666984558105\n",
            "590 0.9666666984558105\n",
            "600 0.9333333969116211\n",
            "610 0.9000000357627869\n",
            "620 0.9333333969116211\n",
            "630 0.9666666984558105\n",
            "640 0.9666666984558105\n",
            "650 0.9333333969116211\n",
            "660 0.9333333969116211\n",
            "670 0.9000000357627869\n",
            "680 0.9333333969116211\n",
            "690 0.9000000357627869\n",
            "700 0.9333333969116211\n",
            "710 0.9666666984558105\n",
            "720 0.9666666984558105\n",
            "730 0.9333333969116211\n",
            "740 0.9333333969116211\n",
            "750 1.0\n",
            "760 0.9666666984558105\n",
            "770 0.9333333969116211\n",
            "780 0.9333333969116211\n",
            "790 0.9000000357627869\n",
            "800 1.0\n",
            "810 0.9000000357627869\n",
            "820 1.0\n",
            "830 0.9000000357627869\n",
            "avg acc: 0.9450040338136595\n",
            "test acc: 0.8848521674422624\n",
            "0 1.0\n",
            "10 1.0\n",
            "20 0.9666666984558105\n",
            "30 0.9666666984558105\n",
            "40 1.0\n",
            "50 1.0\n",
            "60 0.9666666984558105\n",
            "70 1.0\n",
            "80 0.9666666984558105\n",
            "100 0.9666666984558105\n",
            "110 0.9666666984558105\n",
            "120 0.9333333969116211\n",
            "130 0.9666666984558105\n",
            "140 0.9666666984558105\n",
            "150 1.0\n",
            "160 0.9666666984558105\n",
            "170 1.0\n",
            "180 1.0\n",
            "190 0.9666666984558105\n",
            "200 0.8666667342185974\n",
            "210 1.0\n",
            "220 0.8666667342185974\n",
            "230 0.9666666984558105\n",
            "240 0.9333333969116211\n",
            "250 0.8333333730697632\n",
            "260 0.9666666984558105\n",
            "270 0.9666666984558105\n",
            "280 0.9000000357627869\n",
            "290 0.9666666984558105\n",
            "300 0.9666666984558105\n",
            "310 0.9333333969116211\n",
            "320 1.0\n",
            "330 0.9666666984558105\n",
            "340 0.9666666984558105\n",
            "350 0.9333333969116211\n",
            "360 0.9000000357627869\n",
            "370 0.8666667342185974\n",
            "380 0.9333333969116211\n",
            "390 0.8333333730697632\n",
            "400 0.9666666984558105\n",
            "410 1.0\n",
            "420 0.9666666984558105\n",
            "430 0.9666666984558105\n",
            "440 1.0\n",
            "450 0.9666666984558105\n",
            "460 0.9333333969116211\n",
            "470 1.0\n",
            "480 0.9666666984558105\n",
            "490 1.0\n",
            "500 0.9666666984558105\n",
            "510 0.9333333969116211\n",
            "520 0.8666667342185974\n",
            "530 0.9666666984558105\n",
            "540 1.0\n",
            "550 1.0\n",
            "560 0.9333333969116211\n",
            "570 0.9333333969116211\n",
            "580 1.0\n",
            "590 0.9666666984558105\n",
            "600 0.9666666984558105\n",
            "610 0.9666666984558105\n",
            "620 0.9666666984558105\n",
            "630 0.9666666984558105\n",
            "640 0.9333333969116211\n",
            "650 0.9000000357627869\n",
            "660 0.9333333969116211\n",
            "670 1.0\n",
            "680 0.9333333969116211\n",
            "690 0.9666666984558105\n",
            "700 0.9333333969116211\n",
            "710 1.0\n",
            "720 0.9333333969116211\n",
            "730 1.0\n",
            "740 0.9666666984558105\n",
            "750 0.9666666984558105\n",
            "760 0.8666667342185974\n",
            "770 0.9000000357627869\n",
            "780 0.8000000715255737\n",
            "790 0.9666666984558105\n",
            "800 0.9666666984558105\n",
            "810 0.8666667342185974\n",
            "820 1.0\n",
            "830 0.9666666984558105\n",
            "avg acc: 0.9509592677334802\n",
            "test acc: 0.8718625588668621\n",
            "0 1.0\n",
            "10 1.0\n",
            "20 0.9666666984558105\n",
            "30 0.9333333969116211\n",
            "40 1.0\n",
            "50 0.9666666984558105\n",
            "60 0.9666666984558105\n",
            "70 0.9666666984558105\n",
            "80 1.0\n",
            "90 0.9333333969116211\n",
            "100 1.0\n",
            "110 0.9666666984558105\n",
            "120 0.9666666984558105\n",
            "130 0.9666666984558105\n",
            "140 0.9666666984558105\n",
            "150 0.9666666984558105\n",
            "160 0.9666666984558105\n",
            "170 1.0\n",
            "180 0.9666666984558105\n",
            "190 0.9000000357627869\n",
            "200 1.0\n",
            "210 1.0\n",
            "220 0.9333333969116211\n",
            "230 1.0\n",
            "240 0.9666666984558105\n",
            "250 1.0\n",
            "260 0.9666666984558105\n",
            "270 0.9666666984558105\n",
            "280 0.9333333969116211\n",
            "290 0.9333333969116211\n",
            "300 0.9666666984558105\n",
            "310 0.9666666984558105\n",
            "320 0.9666666984558105\n",
            "330 0.9333333969116211\n",
            "340 1.0\n",
            "350 0.9333333969116211\n",
            "360 0.9666666984558105\n",
            "370 0.9333333969116211\n",
            "380 0.9666666984558105\n",
            "390 0.9333333969116211\n",
            "400 0.9666666984558105\n",
            "410 0.9666666984558105\n",
            "420 0.9666666984558105\n",
            "430 0.9333333969116211\n",
            "440 0.9333333969116211\n",
            "450 0.9666666984558105\n",
            "460 1.0\n",
            "470 1.0\n",
            "480 0.9666666984558105\n",
            "490 0.9333333969116211\n",
            "500 0.9666666984558105\n",
            "510 0.9333333969116211\n",
            "520 0.9666666984558105\n",
            "530 0.9666666984558105\n",
            "540 1.0\n",
            "550 0.9666666984558105\n",
            "560 0.9333333969116211\n",
            "570 1.0\n",
            "580 0.9666666984558105\n",
            "590 0.9666666984558105\n",
            "600 1.0\n",
            "610 0.9000000357627869\n",
            "620 0.9333333969116211\n",
            "630 0.9333333969116211\n",
            "640 0.9333333969116211\n",
            "650 0.9666666984558105\n",
            "660 0.9000000357627869\n",
            "670 0.9000000357627869\n",
            "680 1.0\n",
            "690 0.9333333969116211\n",
            "700 0.9666666984558105\n",
            "710 0.8000000715255737\n",
            "720 0.9333333969116211\n",
            "730 0.8666667342185974\n",
            "740 0.9333333969116211\n",
            "750 0.9666666984558105\n",
            "760 1.0\n",
            "770 0.9333333969116211\n",
            "780 0.9000000357627869\n",
            "790 0.9666666984558105\n",
            "800 0.9333333969116211\n",
            "810 0.8666667342185974\n",
            "820 0.9000000357627869\n",
            "830 0.9666666984558105\n",
            "avg acc: 0.9605116213111283\n",
            "test acc: 0.8822142779827118\n",
            "0 0.9666666984558105\n",
            "10 0.9666666984558105\n",
            "20 1.0\n",
            "30 0.9666666984558105\n",
            "40 1.0\n",
            "50 0.9666666984558105\n",
            "60 1.0\n",
            "70 0.9000000357627869\n",
            "80 1.0\n",
            "90 0.9666666984558105\n",
            "100 0.9333333969116211\n",
            "110 1.0\n",
            "120 1.0\n",
            "130 0.9666666984558105\n",
            "140 0.9666666984558105\n",
            "150 1.0\n",
            "160 0.9666666984558105\n",
            "170 0.9333333969116211\n",
            "180 0.9666666984558105\n",
            "190 0.9333333969116211\n",
            "200 0.9666666984558105\n",
            "210 1.0\n",
            "220 0.9666666984558105\n",
            "230 1.0\n",
            "240 0.9666666984558105\n",
            "250 1.0\n",
            "260 0.9333333969116211\n",
            "270 0.9666666984558105\n",
            "280 0.9000000357627869\n",
            "290 1.0\n",
            "300 0.9333333969116211\n",
            "310 0.9666666984558105\n",
            "320 0.9666666984558105\n",
            "330 0.9333333969116211\n",
            "340 1.0\n",
            "350 0.9333333969116211\n",
            "360 0.9666666984558105\n",
            "370 1.0\n",
            "380 1.0\n",
            "390 0.9000000357627869\n",
            "400 1.0\n",
            "410 1.0\n",
            "420 1.0\n",
            "430 1.0\n",
            "440 1.0\n",
            "450 0.9666666984558105\n",
            "460 0.9000000357627869\n",
            "470 1.0\n",
            "480 1.0\n",
            "490 0.8666667342185974\n",
            "500 1.0\n",
            "510 1.0\n",
            "520 1.0\n",
            "530 0.9666666984558105\n",
            "540 0.9000000357627869\n",
            "550 1.0\n",
            "560 0.9333333969116211\n",
            "570 0.9666666984558105\n",
            "580 1.0\n",
            "590 0.9666666984558105\n",
            "600 0.9333333969116211\n",
            "610 0.9666666984558105\n",
            "620 0.9666666984558105\n",
            "630 1.0\n",
            "640 0.9000000357627869\n",
            "650 0.9666666984558105\n",
            "660 1.0\n",
            "670 0.9000000357627869\n",
            "680 0.9333333969116211\n",
            "690 1.0\n",
            "700 1.0\n",
            "710 1.0\n",
            "720 0.9666666984558105\n",
            "730 1.0\n",
            "740 1.0\n",
            "750 1.0\n",
            "760 1.0\n",
            "770 0.8666667342185974\n",
            "780 0.9666666984558105\n",
            "790 0.9333333969116211\n",
            "800 0.9666666984558105\n",
            "810 1.0\n",
            "820 1.0\n",
            "830 0.9666666984558105\n",
            "avg acc: 0.9653077817363419\n",
            "test acc: 0.8769784666222634\n",
            "0 1.0\n",
            "10 0.9666666984558105\n",
            "20 1.0\n",
            "30 0.9333333969116211\n",
            "40 1.0\n",
            "50 1.0\n",
            "60 1.0\n",
            "70 1.0\n",
            "80 0.9666666984558105\n",
            "90 0.9333333969116211\n",
            "100 0.9666666984558105\n",
            "110 0.9666666984558105\n",
            "120 1.0\n",
            "130 0.9666666984558105\n",
            "140 1.0\n",
            "150 1.0\n",
            "160 0.9666666984558105\n",
            "170 1.0\n",
            "180 0.9333333969116211\n",
            "190 1.0\n",
            "200 0.9666666984558105\n",
            "210 1.0\n",
            "220 0.8333333730697632\n",
            "230 1.0\n",
            "240 1.0\n",
            "250 0.9666666984558105\n",
            "260 0.9666666984558105\n",
            "270 0.9000000357627869\n",
            "280 0.9666666984558105\n",
            "290 0.9333333969116211\n",
            "300 0.9666666984558105\n",
            "310 0.9666666984558105\n",
            "320 0.9333333969116211\n",
            "330 1.0\n",
            "340 1.0\n",
            "350 0.9333333969116211\n",
            "360 0.9666666984558105\n",
            "370 0.9666666984558105\n",
            "380 0.9666666984558105\n",
            "390 1.0\n",
            "400 0.9333333969116211\n",
            "410 0.9333333969116211\n",
            "420 1.0\n",
            "430 0.9666666984558105\n",
            "440 0.9666666984558105\n",
            "450 0.9333333969116211\n",
            "460 1.0\n",
            "470 0.9666666984558105\n",
            "480 1.0\n",
            "490 1.0\n",
            "500 0.9333333969116211\n",
            "510 0.9666666984558105\n",
            "520 1.0\n",
            "530 0.9333333969116211\n",
            "540 0.9666666984558105\n",
            "550 0.9333333969116211\n",
            "560 0.9333333969116211\n",
            "570 0.9333333969116211\n",
            "580 1.0\n",
            "590 1.0\n",
            "600 0.9333333969116211\n",
            "610 0.9666666984558105\n",
            "620 1.0\n",
            "630 1.0\n",
            "640 1.0\n",
            "650 0.9666666984558105\n",
            "660 1.0\n",
            "670 1.0\n",
            "680 1.0\n",
            "690 0.9333333969116211\n",
            "700 1.0\n",
            "710 0.9333333969116211\n",
            "720 1.0\n",
            "730 1.0\n",
            "740 0.9666666984558105\n",
            "750 0.9000000357627869\n",
            "760 0.9000000357627869\n",
            "770 0.9333333969116211\n",
            "780 0.9666666984558105\n",
            "790 1.0\n",
            "800 1.0\n",
            "810 0.9666666984558105\n",
            "820 0.9666666984558105\n",
            "830 1.0\n",
            "avg acc: 0.9697442299885144\n",
            "test acc: 0.8815348212667506\n",
            "0 0.9666666984558105\n",
            "10 0.9666666984558105\n",
            "20 0.9666666984558105\n",
            "30 0.9666666984558105\n",
            "40 0.9666666984558105\n",
            "50 0.8666667342185974\n",
            "60 1.0\n",
            "70 1.0\n",
            "80 1.0\n",
            "90 1.0\n",
            "100 1.0\n",
            "110 0.9666666984558105\n",
            "120 1.0\n",
            "130 1.0\n",
            "140 1.0\n",
            "150 0.9666666984558105\n",
            "160 1.0\n",
            "170 0.9333333969116211\n",
            "180 1.0\n",
            "190 0.9000000357627869\n",
            "200 1.0\n",
            "210 0.8666667342185974\n",
            "220 1.0\n",
            "230 1.0\n",
            "240 1.0\n",
            "250 0.9000000357627869\n",
            "260 1.0\n",
            "270 1.0\n",
            "280 0.9666666984558105\n",
            "290 0.9666666984558105\n",
            "300 0.9666666984558105\n",
            "310 0.9666666984558105\n",
            "320 0.9666666984558105\n",
            "330 1.0\n",
            "340 1.0\n",
            "350 0.9333333969116211\n",
            "360 0.9666666984558105\n",
            "370 1.0\n",
            "380 0.9666666984558105\n",
            "390 1.0\n",
            "400 0.9666666984558105\n",
            "410 1.0\n",
            "420 1.0\n",
            "430 1.0\n",
            "440 1.0\n",
            "450 1.0\n",
            "460 1.0\n",
            "470 1.0\n",
            "480 0.9666666984558105\n",
            "490 1.0\n",
            "500 1.0\n",
            "510 1.0\n",
            "520 0.9666666984558105\n",
            "530 0.9666666984558105\n",
            "540 0.9666666984558105\n",
            "550 0.9000000357627869\n",
            "560 0.9000000357627869\n",
            "570 0.9666666984558105\n",
            "580 1.0\n",
            "590 0.9666666984558105\n",
            "600 1.0\n",
            "610 0.9666666984558105\n",
            "620 1.0\n",
            "630 0.9666666984558105\n",
            "640 0.9666666984558105\n",
            "650 1.0\n",
            "660 0.9666666984558105\n",
            "670 1.0\n",
            "680 0.9666666984558105\n",
            "690 0.9666666984558105\n",
            "700 0.9666666984558105\n",
            "710 1.0\n",
            "720 0.8666667342185974\n",
            "730 1.0\n",
            "740 0.9666666984558105\n",
            "750 0.9333333969116211\n",
            "760 1.0\n",
            "770 0.9666666984558105\n",
            "780 1.0\n",
            "790 0.9666666984558105\n",
            "800 1.0\n",
            "810 1.0\n",
            "820 1.0\n",
            "830 0.9333333969116211\n",
            "avg acc: 0.9726618941453435\n",
            "test acc: 0.8754996503714463\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6Gzb78B8B1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predice_test(x):\n",
        "\n",
        "  preds = torch.round(torch.sigmoid(x))\n",
        "  return preds"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXxZTxQY8BzW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "38c7e7d5-3710-47f2-8c44-f59918a77708"
      },
      "source": [
        "for batch in test_iterator:\n",
        "  pred = rnn(batch.text).squeeze(1)\n",
        "  pred = predice_test(pred)\n",
        "  print(pred)\n",
        "  print(batch.label)\n",
        "  break\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.], device='cuda:0',\n",
            "       grad_fn=<RoundBackward>)\n",
            "tensor([1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0.], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT2nZTLjb5nW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}